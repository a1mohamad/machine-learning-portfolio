{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:01:24.403404Z",
     "iopub.status.busy": "2025-10-24T15:01:24.402832Z",
     "iopub.status.idle": "2025-10-24T15:01:24.423221Z",
     "shell.execute_reply": "2025-10-24T15:01:24.421958Z",
     "shell.execute_reply.started": "2025-10-24T15:01:24.403375Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "import keras_cv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"TF_CPP_MIN_VLOG_LEVEL\"] = \"0\"\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:01:28.419500Z",
     "iopub.status.busy": "2025-10-24T15:01:28.419140Z",
     "iopub.status.idle": "2025-10-24T15:01:28.428568Z",
     "shell.execute_reply": "2025-10-24T15:01:28.427450Z",
     "shell.execute_reply.started": "2025-10-24T15:01:28.419478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: \n",
      "\n",
      "/device:CPU:0 CPU\n"
     ]
    }
   ],
   "source": [
    "print(\"Available devices: \\n\")\n",
    "for device in tf.config.list_logical_devices():\n",
    "    print(device.name, device.device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:01:29.429783Z",
     "iopub.status.busy": "2025-10-24T15:01:29.429447Z",
     "iopub.status.idle": "2025-10-24T15:01:29.842188Z",
     "shell.execute_reply": "2025-10-24T15:01:29.841135Z",
     "shell.execute_reply.started": "2025-10-24T15:01:29.429737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No TPU/GPU found. Using CPU strategy: _DefaultDistributionStrategy\n",
      "REPLICAS: 1\n"
     ]
    }
   ],
   "source": [
    "def get_strategy():\n",
    "    \"\"\"\n",
    "    Detects and returns the best TensorFlow distribution strategy.\n",
    "    - TPUStrategy for TPU(s)\n",
    "    - MirroredStrategy for GPU(s)\n",
    "    - Default strategy for CPU\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try TPU first\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.TPUStrategy(tpu)\n",
    "        print(\"Using TPU strategy:\", type(strategy).__name__)\n",
    "    except Exception:\n",
    "        # If TPU not available, try GPU\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            strategy = tf.distribute.MirroredStrategy()\n",
    "            print(\"Using GPU strategy:\", type(strategy).__name__)\n",
    "        else:\n",
    "            # Fallback CPU\n",
    "            strategy = tf.distribute.get_strategy()\n",
    "            print(\"No TPU/GPU found. Using CPU strategy:\", type(strategy).__name__)\n",
    "\n",
    "    print(\"REPLICAS:\", strategy.num_replicas_in_sync)\n",
    "    return strategy\n",
    "\n",
    "# Call it\n",
    "strategy = get_strategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:01:32.271307Z",
     "iopub.status.busy": "2025-10-24T15:01:32.270954Z",
     "iopub.status.idle": "2025-10-24T15:01:32.276358Z",
     "shell.execute_reply": "2025-10-24T15:01:32.275439Z",
     "shell.execute_reply.started": "2025-10-24T15:01:32.271282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS: 1\n",
      "TensorFlow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "print(\"REPLICAS:\", strategy.num_replicas_in_sync)\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:01:33.308471Z",
     "iopub.status.busy": "2025-10-24T15:01:33.307525Z",
     "iopub.status.idle": "2025-10-24T15:01:33.314678Z",
     "shell.execute_reply": "2025-10-24T15:01:33.313399Z",
     "shell.execute_reply.started": "2025-10-24T15:01:33.308443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For reproducing purposes, everything seeded !\n"
     ]
    }
   ],
   "source": [
    "SEED = 28\n",
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    print('For reproducing purposes, everything seeded !')\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:01:34.653976Z",
     "iopub.status.busy": "2025-10-24T15:01:34.653581Z",
     "iopub.status.idle": "2025-10-24T15:01:34.658965Z",
     "shell.execute_reply": "2025-10-24T15:01:34.657722Z",
     "shell.execute_reply.started": "2025-10-24T15:01:34.653922Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/kaggle/input/global-wheat-detection'\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "TEST_DIR = os.path.join(DATA_DIR, 'test')\n",
    "CSV_PATH = os.path.join(DATA_DIR, 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:01:35.801718Z",
     "iopub.status.busy": "2025-10-24T15:01:35.800633Z",
     "iopub.status.idle": "2025-10-24T15:01:35.846081Z",
     "shell.execute_reply": "2025-10-24T15:01:35.845117Z",
     "shell.execute_reply.started": "2025-10-24T15:01:35.801679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total images on Train directory: 3422\n",
      "Number of test images on Test directory: 10\n"
     ]
    }
   ],
   "source": [
    "num_train_images = len(os.listdir(TRAIN_DIR))\n",
    "num_test_images = len(os.listdir(TEST_DIR))\n",
    "print(f'Number of total images on Train directory: {num_train_images}')\n",
    "print(f'Number of test images on Test directory: {num_test_images}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:01:36.725592Z",
     "iopub.status.busy": "2025-10-24T15:01:36.725294Z",
     "iopub.status.idle": "2025-10-24T15:01:36.783824Z",
     "shell.execute_reply": "2025-10-24T15:01:36.782822Z",
     "shell.execute_reply.started": "2025-10-24T15:01:36.725571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "img_path = os.path.join(TRAIN_DIR, os.listdir(TRAIN_DIR)[0])\n",
    "img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:01:38.443085Z",
     "iopub.status.busy": "2025-10-24T15:01:38.442707Z",
     "iopub.status.idle": "2025-10-24T15:01:38.675876Z",
     "shell.execute_reply": "2025-10-24T15:01:38.674777Z",
     "shell.execute_reply.started": "2025-10-24T15:01:38.443059Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>bbox</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[834.0, 222.0, 56.0, 36.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[226.0, 548.0, 130.0, 58.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[377.0, 504.0, 74.0, 160.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[834.0, 95.0, 109.0, 107.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[26.0, 144.0, 124.0, 117.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id  width  height                         bbox   source\n",
       "0  b6ab77fd7   1024    1024   [834.0, 222.0, 56.0, 36.0]  usask_1\n",
       "1  b6ab77fd7   1024    1024  [226.0, 548.0, 130.0, 58.0]  usask_1\n",
       "2  b6ab77fd7   1024    1024  [377.0, 504.0, 74.0, 160.0]  usask_1\n",
       "3  b6ab77fd7   1024    1024  [834.0, 95.0, 109.0, 107.0]  usask_1\n",
       "4  b6ab77fd7   1024    1024  [26.0, 144.0, 124.0, 117.0]  usask_1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T16:41:04.217294Z",
     "iopub.status.busy": "2025-10-21T16:41:04.216947Z",
     "iopub.status.idle": "2025-10-21T16:41:04.222603Z",
     "shell.execute_reply": "2025-10-21T16:41:04.221767Z",
     "shell.execute_reply.started": "2025-10-21T16:41:04.217271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147793, 5)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T15:43:43.544696Z",
     "iopub.status.busy": "2025-10-21T15:43:43.544260Z",
     "iopub.status.idle": "2025-10-21T15:43:43.572804Z",
     "shell.execute_reply": "2025-10-21T15:43:43.571415Z",
     "shell.execute_reply.started": "2025-10-21T15:43:43.544671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Bounding boxes exists in an image: 43\n"
     ]
    }
   ],
   "source": [
    "averaged_bbox_per_img = df.groupby('image_id').size().mean()\n",
    "print(f'Average Bounding boxes exists in an image: {int(averaged_bbox_per_img)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T15:04:22.992131Z",
     "iopub.status.busy": "2025-10-18T15:04:22.991404Z",
     "iopub.status.idle": "2025-10-18T15:04:23.014126Z",
     "shell.execute_reply": "2025-10-18T15:04:23.013523Z",
     "shell.execute_reply.started": "2025-10-18T15:04:22.992106Z"
    }
   },
   "outputs": [],
   "source": [
    "bbox_counts = df.groupby('image_id').size()\n",
    "print('Statistics of wheat head per image:')\n",
    "print(bbox_counts.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (12, 6))\n",
    "sns.histplot(bbox_counts, bins= 30, kde= True, color= 'purple')\n",
    "plt.title('Number of Bounding Boxes per Image')\n",
    "plt.xlabel('Number of Bounding Boxes')\n",
    "plt.ylabel('Number of images')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:01:47.406425Z",
     "iopub.status.busy": "2025-10-24T15:01:47.406083Z",
     "iopub.status.idle": "2025-10-24T15:01:47.419918Z",
     "shell.execute_reply": "2025-10-24T15:01:47.419089Z",
     "shell.execute_reply.started": "2025-10-24T15:01:47.406401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images with Wheat: 3373\n"
     ]
    }
   ],
   "source": [
    "annonated_ids = set(df['image_id'].unique())\n",
    "print(f'Number of images with Wheat: {len(annonated_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:01:48.869403Z",
     "iopub.status.busy": "2025-10-24T15:01:48.869066Z",
     "iopub.status.idle": "2025-10-24T15:01:48.879103Z",
     "shell.execute_reply": "2025-10-24T15:01:48.878197Z",
     "shell.execute_reply.started": "2025-10-24T15:01:48.869379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images without annonation(Wheat): 49\n",
      "Example of empty image: dec23c826\n"
     ]
    }
   ],
   "source": [
    "all_images = [f.replace('.jpg', '') for f in os.listdir(TRAIN_DIR)]\n",
    "empty_images = [f for f in all_images if f not in annonated_ids]\n",
    "print(f'Number of images without annonation(Wheat): {len(empty_images)}')\n",
    "print(f'Example of empty image: {empty_images[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T15:04:30.048023Z",
     "iopub.status.busy": "2025-10-18T15:04:30.047461Z",
     "iopub.status.idle": "2025-10-18T15:04:30.054573Z",
     "shell.execute_reply": "2025-10-18T15:04:30.053965Z",
     "shell.execute_reply.started": "2025-10-18T15:04:30.048000Z"
    }
   },
   "outputs": [],
   "source": [
    "empty_img_frac = len(empty_images) / len(os.listdir(TRAIN_DIR))\n",
    "annonated_img_frac = len(annonated_ids) / len(os.listdir(TRAIN_DIR))\n",
    "\n",
    "print(f'Empty images percentage: {empty_img_frac:.4f}')\n",
    "print(f'Annonated images percentage: {annonated_img_frac:.4f}')\n",
    "print(\"Empty images aren't dominated, no problem with them at all!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.path.join(TRAIN_DIR, empty_images[0] + '.jpg')\n",
    "img = Image.open(img_path)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(f'Example of empty: {empty_images[0]}.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:46:54.208484Z",
     "iopub.status.busy": "2025-09-27T16:46:54.207478Z",
     "iopub.status.idle": "2025-09-27T16:46:54.213407Z",
     "shell.execute_reply": "2025-09-27T16:46:54.212691Z",
     "shell.execute_reply.started": "2025-09-27T16:46:54.208459Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_images(num_images= 6, cols= 3):\n",
    "    files = os.listdir(TRAIN_DIR)[:num_images]\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "\n",
    "    fig = plt.figure(figsize= (cols* 4, rows* 4))\n",
    "    \n",
    "    for i, fname in enumerate(files):\n",
    "        img_path = os.path.join(TRAIN_DIR, fname)\n",
    "        img = Image.open(img_path)\n",
    "        img = img.resize((256, 256))\n",
    "\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(fname)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T17:26:57.010802Z",
     "iopub.status.busy": "2025-10-20T17:26:57.009817Z",
     "iopub.status.idle": "2025-10-20T17:26:57.030793Z",
     "shell.execute_reply": "2025-10-20T17:26:57.029634Z",
     "shell.execute_reply.started": "2025-10-20T17:26:57.010765Z"
    }
   },
   "outputs": [],
   "source": [
    "show_images(num_images= 6, cols= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:01:51.490989Z",
     "iopub.status.busy": "2025-10-24T15:01:51.490563Z",
     "iopub.status.idle": "2025-10-24T15:01:53.520050Z",
     "shell.execute_reply": "2025-10-24T15:01:53.519079Z",
     "shell.execute_reply.started": "2025-10-24T15:01:51.490957Z"
    }
   },
   "outputs": [],
   "source": [
    "df['bbox'] = df['bbox'].apply(ast.literal_eval)\n",
    "df['x_min'] = df['bbox'].apply(lambda b: b[0])\n",
    "df['y_min'] = df['bbox'].apply(lambda b: b[1])\n",
    "df['x_max'] = df['bbox'].apply(lambda b: b[0] + b[2])\n",
    "df['y_max'] = df['bbox'].apply(lambda b: b[1] + b[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T10:18:02.175448Z",
     "iopub.status.busy": "2025-09-22T10:18:02.174901Z",
     "iopub.status.idle": "2025-09-22T10:18:02.187524Z",
     "shell.execute_reply": "2025-09-22T10:18:02.186900Z",
     "shell.execute_reply.started": "2025-09-22T10:18:02.175426Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T15:43:53.369058Z",
     "iopub.status.busy": "2025-10-21T15:43:53.368751Z",
     "iopub.status.idle": "2025-10-21T15:43:53.419940Z",
     "shell.execute_reply": "2025-10-21T15:43:53.419167Z",
     "shell.execute_reply.started": "2025-10-21T15:43:53.369035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           count       mean        std  min   25%   50%    75%    max\n",
      "width   147793.0  84.435060  35.553450  1.0  62.0  78.0  100.0  987.0\n",
      "height  147793.0  76.927306  33.853186  1.0  55.0  71.0   91.0  714.0\n"
     ]
    }
   ],
   "source": [
    "df['width'] = df['x_max'] - df['x_min']\n",
    "df['height'] = df['y_max'] - df['y_min']\n",
    "print(df[['width' ,'height']].describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:47:47.736376Z",
     "iopub.status.busy": "2025-09-27T16:47:47.736098Z",
     "iopub.status.idle": "2025-09-27T16:47:47.748312Z",
     "shell.execute_reply": "2025-09-27T16:47:47.747669Z",
     "shell.execute_reply.started": "2025-09-27T16:47:47.736354Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize= (12, 6))\n",
    "for i, col in enumerate(['width', 'height']):\n",
    "    sns.histplot(df[col], bins= 50, kde= True, ax= ax[i])\n",
    "    ax[i].set_title(f'Bounding Boxes {col} distribution')\n",
    "    ax[i].set_xlim((0, 250))\n",
    "    ax[i].set_xlabel(f'{col} pixels')\n",
    "    ax[i].set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T16:48:04.397849Z",
     "iopub.status.busy": "2025-09-27T16:48:04.397647Z",
     "iopub.status.idle": "2025-09-27T16:48:04.404318Z",
     "shell.execute_reply": "2025-09-27T16:48:04.403463Z",
     "shell.execute_reply.started": "2025-09-27T16:48:04.397832Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_images_with_bboxes(df, image_dir, nrows, ncols):\n",
    "    # Pick random images from the train dir\n",
    "    files = os.listdir(image_dir)\n",
    "    selected_files = random.sample(files, nrows * ncols)\n",
    "\n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize=(4*ncols, 4*nrows))\n",
    "\n",
    "    for ax, fname in zip(axs.flatten(), selected_files):\n",
    "        image_id = fname.replace('.jpg', '')\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(image_dir, fname)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Get bboxes if exists\n",
    "        if image_id in df['image_id'].values:\n",
    "            bboxes = df[df['image_id'] == image_id][['x_min', 'y_min', 'x_max', 'y_max']].values\n",
    "            for (x_min, y_min, x_max, y_max) in bboxes:\n",
    "                start_point = (int(x_min), int(y_min))\n",
    "                end_point = (int(x_max), int(y_max))\n",
    "                color = (255, 0, 0)\n",
    "                thickness = 2\n",
    "                cv2.rectangle(img, start_point, end_point, color, thickness)\n",
    "\n",
    "        # Show image\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(fname, fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_with_bboxes(df, TRAIN_DIR, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:01:55.983103Z",
     "iopub.status.busy": "2025-10-24T15:01:55.982798Z",
     "iopub.status.idle": "2025-10-24T15:01:56.620904Z",
     "shell.execute_reply": "2025-10-24T15:01:56.619936Z",
     "shell.execute_reply.started": "2025-10-24T15:01:55.983082Z"
    }
   },
   "outputs": [],
   "source": [
    "grouped = df.groupby('image_id')[['x_min', 'y_min', 'x_max', 'y_max']].apply(\n",
    "    lambda x: x.values.tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:01:56.622515Z",
     "iopub.status.busy": "2025-10-24T15:01:56.622228Z",
     "iopub.status.idle": "2025-10-24T15:01:56.714807Z",
     "shell.execute_reply": "2025-10-24T15:01:56.713984Z",
     "shell.execute_reply.started": "2025-10-24T15:01:56.622488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'image_path': '/kaggle/input/global-wheat-detection/train/00333207f.jpg', 'bboxes': array([[   0.,  654.,   37.,  765.],\n",
      "       [   0.,  817.,  135.,  915.],\n",
      "       [   0.,  192.,   22.,  273.],\n",
      "       [   4.,  342.,   67.,  380.],\n",
      "       [  82.,  334.,  164.,  415.],\n",
      "       [  30.,  296.,   78.,  345.],\n",
      "       [ 176.,  316.,  246.,  370.],\n",
      "       [ 176.,  126.,  245.,  177.],\n",
      "       [ 203.,   38.,  245.,  123.],\n",
      "       [   3.,  142.,   92.,  200.],\n",
      "       [ 236.,    0.,  296.,   25.],\n",
      "       [ 329.,    0.,  404.,   57.],\n",
      "       [ 796.,    0.,  865.,   96.],\n",
      "       [ 659.,   24.,  718.,  114.],\n",
      "       [ 540.,   81.,  680.,  161.],\n",
      "       [ 233.,  152.,  322.,  203.],\n",
      "       [ 422.,  159.,  480.,  209.],\n",
      "       [ 462.,  153.,  667.,  217.],\n",
      "       [ 468.,  210.,  576.,  263.],\n",
      "       [ 417.,  235.,  553.,  323.],\n",
      "       [ 287.,  257.,  343.,  308.],\n",
      "       [ 283.,  322.,  400.,  398.],\n",
      "       [ 393.,  329.,  567.,  429.],\n",
      "       [ 606.,  346.,  653.,  403.],\n",
      "       [ 611.,  286.,  681.,  342.],\n",
      "       [ 718.,  305.,  772.,  374.],\n",
      "       [ 709.,  179.,  811.,  259.],\n",
      "       [ 813.,  191.,  933.,  256.],\n",
      "       [ 862.,  121.,  927.,  173.],\n",
      "       [ 876.,  400.,  956.,  504.],\n",
      "       [ 951.,  422., 1003.,  477.],\n",
      "       [ 763.,  414.,  832.,  468.],\n",
      "       [ 633.,  462.,  710.,  507.],\n",
      "       [ 438.,  436.,  542.,  487.],\n",
      "       [ 356.,  448.,  421.,  498.],\n",
      "       [ 292.,  418.,  361.,  497.],\n",
      "       [ 251.,  528.,  326.,  590.],\n",
      "       [ 421.,  501.,  473.,  550.],\n",
      "       [ 692.,  487.,  769.,  553.],\n",
      "       [ 769.,  474.,  859.,  575.],\n",
      "       [ 692.,  685.,  775.,  743.],\n",
      "       [ 611.,  710.,  683.,  802.],\n",
      "       [ 417.,  635.,  487.,  706.],\n",
      "       [ 706.,  768.,  809.,  825.],\n",
      "       [ 820.,  755.,  947.,  811.],\n",
      "       [ 899.,  730.,  982.,  782.],\n",
      "       [ 855.,  850., 1009., 1004.],\n",
      "       [ 792.,  939.,  855., 1024.],\n",
      "       [ 605.,  875.,  780.,  965.],\n",
      "       [ 364.,  832.,  422.,  896.],\n",
      "       [ 246.,  929.,  385., 1006.],\n",
      "       [ 400.,  937.,  482., 1024.],\n",
      "       [ 471.,  899.,  534.,  959.],\n",
      "       [ 701.,  506.,  863.,  593.],\n",
      "       [ 552.,  404.,  639.,  478.]], dtype=float32)}, {'image_path': '/kaggle/input/global-wheat-detection/train/005b0d8bb.jpg', 'bboxes': array([[7.650e+02, 8.790e+02, 8.810e+02, 9.580e+02],\n",
      "       [8.400e+01, 5.390e+02, 2.370e+02, 6.350e+02],\n",
      "       [8.550e+02, 7.810e+02, 9.510e+02, 8.570e+02],\n",
      "       [7.980e+02, 9.340e+02, 9.310e+02, 1.022e+03],\n",
      "       [3.300e+02, 3.950e+02, 4.750e+02, 6.100e+02],\n",
      "       [0.000e+00, 0.000e+00, 7.200e+01, 1.060e+02],\n",
      "       [7.110e+02, 6.000e+01, 8.490e+02, 1.570e+02],\n",
      "       [1.570e+02, 3.930e+02, 3.770e+02, 5.010e+02],\n",
      "       [9.670e+02, 5.450e+02, 1.024e+03, 6.840e+02],\n",
      "       [3.050e+02, 5.770e+02, 5.250e+02, 6.800e+02],\n",
      "       [5.620e+02, 5.180e+02, 7.560e+02, 6.280e+02],\n",
      "       [7.370e+02, 5.550e+02, 8.600e+02, 7.360e+02],\n",
      "       [2.120e+02, 1.510e+02, 3.960e+02, 2.670e+02],\n",
      "       [6.060e+02, 2.290e+02, 7.730e+02, 3.790e+02],\n",
      "       [3.220e+02, 8.600e+02, 4.060e+02, 1.024e+03],\n",
      "       [9.330e+02, 4.300e+01, 1.022e+03, 2.750e+02],\n",
      "       [1.920e+02, 2.890e+02, 4.540e+02, 3.870e+02],\n",
      "       [1.000e+00, 6.430e+02, 8.100e+01, 7.310e+02],\n",
      "       [9.700e+01, 4.770e+02, 2.230e+02, 5.330e+02],\n",
      "       [1.000e+00, 3.990e+02, 8.100e+01, 5.070e+02]], dtype=float32)}]\n"
     ]
    }
   ],
   "source": [
    "data_dicts = []\n",
    "for image_id, bboxes in grouped.items():\n",
    "    img_path = os.path.join(TRAIN_DIR, f'{image_id}.jpg')\n",
    "    bboxes = np.array(bboxes, dtype=np.float32).reshape(-1, 4)\n",
    "    data_dicts .append({\n",
    "        'image_path': img_path,\n",
    "         'bboxes': bboxes\n",
    "    })\n",
    "\n",
    "print(data_dicts[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:01:59.252445Z",
     "iopub.status.busy": "2025-10-24T15:01:59.251633Z",
     "iopub.status.idle": "2025-10-24T15:01:59.262624Z",
     "shell.execute_reply": "2025-10-24T15:01:59.261800Z",
     "shell.execute_reply.started": "2025-10-24T15:01:59.252408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Validation dicts created successfully! 20% of data stored for validation\n"
     ]
    }
   ],
   "source": [
    "train_dicts, val_dicts = train_test_split(\n",
    "    data_dicts,\n",
    "    test_size= 0.2,\n",
    "    random_state= SEED,\n",
    "    shuffle= True\n",
    ")\n",
    "print('Train and Validation dicts created successfully! 20% of data stored for validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:01:59.963722Z",
     "iopub.status.busy": "2025-10-24T15:01:59.963424Z",
     "iopub.status.idle": "2025-10-24T15:01:59.970059Z",
     "shell.execute_reply": "2025-10-24T15:01:59.969119Z",
     "shell.execute_reply.started": "2025-10-24T15:01:59.963700Z"
    }
   },
   "outputs": [],
   "source": [
    "for fname in empty_images:\n",
    "    img_path = os.path.join(TRAIN_DIR, f'{fname}.jpg')\n",
    "    bboxes = np.zeros((0, 4), dtype=np.float32)\n",
    "    train_dicts.append({\n",
    "        'image_path': img_path,\n",
    "        'bboxes': bboxes\n",
    "    })\n",
    "\n",
    "random.shuffle(train_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:17:45.365319Z",
     "iopub.status.busy": "2025-10-24T15:17:45.364975Z",
     "iopub.status.idle": "2025-10-24T15:17:45.372457Z",
     "shell.execute_reply": "2025-10-24T15:17:45.371218Z",
     "shell.execute_reply.started": "2025-10-24T15:17:45.365296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Batch size: 4\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = (1024, 1024)\n",
    "NUM_CLASSES = 1\n",
    "GLOBAL_CLIPNORM = 10.0\n",
    "WARMUP_LR= 1e-3\n",
    "FINE_TUNE_BB_LR = 5e-5\n",
    "FINE_TUNE_MODEL_LR = 1e-5\n",
    "WARMUP_EPOCH = 10\n",
    "INTERMEDIATE_EPOCH = WARMUP_EPOCH + 20\n",
    "FINAL_EPOCH = INTERMEDIATE_EPOCH + 50\n",
    "MAX_BOXES = 120\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "BATCH_SIZE_PER_REPLICA = 4\n",
    "BUFFER_SHUFFLE_SIZE = 512\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "print(f'Global Batch size: {BATCH_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:17:45.532360Z",
     "iopub.status.busy": "2025-10-24T15:17:45.532040Z",
     "iopub.status.idle": "2025-10-24T15:17:45.538772Z",
     "shell.execute_reply": "2025-10-24T15:17:45.537832Z",
     "shell.execute_reply.started": "2025-10-24T15:17:45.532336Z"
    }
   },
   "outputs": [],
   "source": [
    "# This generator will read one image and its boxes at a time\n",
    "def data_generator(dict_list):\n",
    "    for sample in dict_list:\n",
    "        image = tf.io.read_file(sample['image_path'])\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image = image / 255.\n",
    "        \n",
    "        # We need to provide the classes alongside the boxes\n",
    "        num_boxes = sample['bboxes'].shape[0]\n",
    "        bounding_boxes = {\n",
    "            'boxes': sample['bboxes'],\n",
    "            'classes': tf.zeros(shape=(num_boxes,), dtype=tf.float32)\n",
    "        }\n",
    "        yield {'images': image, 'bounding_boxes': bounding_boxes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:17:47.384683Z",
     "iopub.status.busy": "2025-10-24T15:17:47.384354Z",
     "iopub.status.idle": "2025-10-24T15:17:47.417292Z",
     "shell.execute_reply": "2025-10-24T15:17:47.416425Z",
     "shell.execute_reply.started": "2025-10-24T15:17:47.384656Z"
    }
   },
   "outputs": [],
   "source": [
    "mosaic = keras_cv.layers.Mosaic(bounding_box_format=\"xyxy\", name= 'mosaic')\n",
    "random_flip = keras_cv.layers.RandomFlip(\n",
    "    mode=\"horizontal\", \n",
    "    bounding_box_format=\"xyxy\"\n",
    ")\n",
    "# Use a gentler scaling factor to avoid making small wheat heads disappear\n",
    "train_resizing = keras_cv.layers.JitteredResize(\n",
    "    target_size=IMG_SIZE, \n",
    "    scale_factor=(0.9, 1.1), \n",
    "    bounding_box_format=\"xyxy\"\n",
    ")\n",
    "\n",
    "val_resizing = keras_cv.layers.JitteredResize(\n",
    "    target_size=IMG_SIZE, \n",
    "    scale_factor=(1.0, 1.0), # Reduced range\n",
    "    bounding_box_format=\"xyxy\"\n",
    ")\n",
    "\n",
    "random_color_jitter = keras_cv.layers.RandomColorJitter(\n",
    "    value_range= (0.0, 1.0),\n",
    "    brightness_factor= 0.2,\n",
    "    contrast_factor= 0.2,\n",
    "    saturation_factor= 0.2,\n",
    "    hue_factor= 0.1\n",
    ")\n",
    "\n",
    "random_color_deg = keras_cv.layers.RandomColorDegeneration(\n",
    "    factor= (0.2, 0.7),\n",
    "    seed= SEED\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:17:48.455691Z",
     "iopub.status.busy": "2025-10-24T15:17:48.455392Z",
     "iopub.status.idle": "2025-10-24T15:17:48.466978Z",
     "shell.execute_reply": "2025-10-24T15:17:48.465916Z",
     "shell.execute_reply.started": "2025-10-24T15:17:48.455667Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- LIGHTER AUGMENTATIONS for Phase 2 and 3 ---\n",
    "\n",
    "# Keep flip and a gentler resize\n",
    "random_flip_light = keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xyxy\")\n",
    "train_resizing_light = keras_cv.layers.JitteredResize(\n",
    "    target_size=IMG_SIZE, \n",
    "    scale_factor=(0.95, 1.05), # Reduced range\n",
    "    bounding_box_format=\"xyxy\"\n",
    ")\n",
    "\n",
    "# Reduce the intensity of color jitter\n",
    "random_color_jitter_light = keras_cv.layers.RandomColorJitter(\n",
    "    value_range=(0.0, 1.0),\n",
    "    brightness_factor=0.1, # Reduced from 0.2\n",
    "    contrast_factor=0.1,   # Reduced from 0.2\n",
    "    saturation_factor=0.1, # Reduced from 0.2\n",
    "    hue_factor=0.05        # Reduced from 0.1\n",
    ")\n",
    "\n",
    "# Reduce the intensity of color degeneration\n",
    "random_color_deg_light = keras_cv.layers.RandomColorDegeneration(\n",
    "    factor=(0.1, 0.4), # Tighter, weaker range than (0.2, 0.7)\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:17:49.340155Z",
     "iopub.status.busy": "2025-10-24T15:17:49.339868Z",
     "iopub.status.idle": "2025-10-24T15:17:49.344741Z",
     "shell.execute_reply": "2025-10-24T15:17:49.343740Z",
     "shell.execute_reply.started": "2025-10-24T15:17:49.340136Z"
    }
   },
   "outputs": [],
   "source": [
    "def augment_strong(inputs):\n",
    "    inputs = train_resizing(inputs)\n",
    "    inputs = random_flip(inputs)\n",
    "    inputs = random_color_jitter(inputs)\n",
    "    inputs = random_color_deg(inputs)\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:17:51.188060Z",
     "iopub.status.busy": "2025-10-24T15:17:51.187729Z",
     "iopub.status.idle": "2025-10-24T15:17:51.193222Z",
     "shell.execute_reply": "2025-10-24T15:17:51.192060Z",
     "shell.execute_reply.started": "2025-10-24T15:17:51.188037Z"
    }
   },
   "outputs": [],
   "source": [
    "# New augmentation pipeline function\n",
    "def augment_light(inputs):\n",
    "    inputs = train_resizing_light(inputs)\n",
    "    inputs = random_flip_light(inputs)\n",
    "    inputs = random_color_jitter_light(inputs)\n",
    "    inputs = random_color_deg_light(inputs)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:19:48.132815Z",
     "iopub.status.busy": "2025-10-24T15:19:48.132470Z",
     "iopub.status.idle": "2025-10-24T15:19:48.139968Z",
     "shell.execute_reply": "2025-10-24T15:19:48.138918Z",
     "shell.execute_reply.started": "2025-10-24T15:19:48.132791Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_boxes_to_max(inputs, max_boxes=MAX_BOXES):\n",
    "    images = inputs['images']\n",
    "    boxes = inputs['bounding_boxes']['boxes']\n",
    "    classes = inputs['bounding_boxes']['classes']\n",
    "\n",
    "    # If ragged, convert to dense with padding\n",
    "    if isinstance(boxes, tf.RaggedTensor):\n",
    "        boxes = boxes.to_tensor(default_value=-1.0, shape=[None, max_boxes, 4])\n",
    "        classes = classes.to_tensor(default_value=-1.0, shape=[None, max_boxes])\n",
    "    else:\n",
    "        # Dense case: truncate/pad manually\n",
    "        num_boxes = tf.shape(boxes)[1]\n",
    "        pad_amount = max_boxes - num_boxes\n",
    "        boxes = boxes[:, :max_boxes]\n",
    "        classes = classes[:, :max_boxes]\n",
    "        boxes = tf.pad(boxes, [[0, 0], [0, pad_amount], [0, 0]], constant_values=-1.0)\n",
    "        classes = tf.pad(classes, [[0, 0], [0, pad_amount]], constant_values=-1.0)\n",
    "\n",
    "    inputs['bounding_boxes']['boxes'] = boxes\n",
    "    inputs['bounding_boxes']['classes'] = classes\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:19:55.981967Z",
     "iopub.status.busy": "2025-10-24T15:19:55.981641Z",
     "iopub.status.idle": "2025-10-24T15:19:55.989349Z",
     "shell.execute_reply": "2025-10-24T15:19:55.988417Z",
     "shell.execute_reply.started": "2025-10-24T15:19:55.981942Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_strong_dataset(dict_list, batch_size=BATCH_SIZE):\n",
    "    \n",
    "    output_signature = {\n",
    "        'images': tf.TensorSpec(shape=(None, None, 3), dtype=tf.float32),\n",
    "        'bounding_boxes': {\n",
    "            'boxes': tf.TensorSpec(shape=(None, 4), dtype=tf.float32),\n",
    "            'classes': tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(dict_list),\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "\n",
    "    ds = ds.shuffle(BUFFER_SHUFFLE_SIZE)\n",
    "    \n",
    "    ds = ds.map(augment_strong, num_parallel_calls= AUTO)\n",
    "    ds = ds.ragged_batch(batch_size= batch_size, drop_remainder= True)\n",
    "    ds = ds.map(mosaic, num_parallel_calls=AUTO)\n",
    "    ds = ds.map(pad_boxes_to_max, num_parallel_calls= AUTO)\n",
    "\n",
    "    ds = ds.map(dict_to_tuple, num_parallel_calls=AUTO)\n",
    "    \n",
    "    return ds.prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:19:56.188279Z",
     "iopub.status.busy": "2025-10-24T15:19:56.187980Z",
     "iopub.status.idle": "2025-10-24T15:19:56.192941Z",
     "shell.execute_reply": "2025-10-24T15:19:56.191856Z",
     "shell.execute_reply.started": "2025-10-24T15:19:56.188258Z"
    }
   },
   "outputs": [],
   "source": [
    "def augment_val(inputs):\n",
    "    # Only applies resizing for validation stability\n",
    "    return val_resizing(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:19:56.453547Z",
     "iopub.status.busy": "2025-10-24T15:19:56.453245Z",
     "iopub.status.idle": "2025-10-24T15:19:56.457920Z",
     "shell.execute_reply": "2025-10-24T15:19:56.456802Z",
     "shell.execute_reply.started": "2025-10-24T15:19:56.453523Z"
    }
   },
   "outputs": [],
   "source": [
    "def dict_to_tuple(inputs):\n",
    "    return inputs['images'], inputs['bounding_boxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:19:57.294716Z",
     "iopub.status.busy": "2025-10-24T15:19:57.294400Z",
     "iopub.status.idle": "2025-10-24T15:19:57.302371Z",
     "shell.execute_reply": "2025-10-24T15:19:57.301130Z",
     "shell.execute_reply.started": "2025-10-24T15:19:57.294691Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_light_dataset(dict_list, batch_size=BATCH_SIZE, is_training= False):\n",
    "    \n",
    "    output_signature = {\n",
    "        'images': tf.TensorSpec(shape=(None, None, 3), dtype=tf.float32),\n",
    "        'bounding_boxes': {\n",
    "            'boxes': tf.TensorSpec(shape=(None, 4), dtype=tf.float32),\n",
    "            'classes': tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(dict_list),\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "\n",
    "    if is_training:\n",
    "        ds = ds.shuffle(BUFFER_SHUFFLE_SIZE)\n",
    "        ds = ds.map(augment_light, num_parallel_calls=AUTO)\n",
    "    else:\n",
    "        ds = ds.map(augment_val, num_parallel_calls=AUTO)\n",
    "\n",
    "    ds = ds.ragged_batch(batch_size= batch_size, drop_remainder= True)\n",
    "    ds = ds.map(pad_boxes_to_max, num_parallel_calls= AUTO)\n",
    "    ds = ds.map(dict_to_tuple, num_parallel_calls=AUTO)\n",
    "    \n",
    "    \n",
    "    return ds.prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:19:58.804837Z",
     "iopub.status.busy": "2025-10-24T15:19:58.804027Z",
     "iopub.status.idle": "2025-10-24T15:20:03.512414Z",
     "shell.execute_reply": "2025-10-24T15:20:03.511559Z",
     "shell.execute_reply.started": "2025-10-24T15:19:58.804809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train and Validation and light augmented Train datasets are ready!\n",
      "Light Augmented dataset for Mid-Tune and Fine-Tune phases created !\n"
     ]
    }
   ],
   "source": [
    "# --- Find this existing code in your notebook ---\n",
    "train_strong_dataset = create_strong_dataset(train_dicts)\n",
    "val_dataset = create_light_dataset(val_dicts, is_training= False)\n",
    "train_light_dataset = create_light_dataset(train_dicts, is_training= True)\n",
    "\n",
    "print('✅ Train and Validation and light augmented Train datasets are ready!')\n",
    "print('Light Augmented dataset for Mid-Tune and Fine-Tune phases created !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:23:00.959335Z",
     "iopub.status.busy": "2025-10-24T15:23:00.958489Z",
     "iopub.status.idle": "2025-10-24T15:23:16.764554Z",
     "shell.execute_reply": "2025-10-24T15:23:16.763463Z",
     "shell.execute_reply.started": "2025-10-24T15:23:00.959305Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1761319381.584852      36 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: -8 } dim { size: 2048 } dim { size: 2048 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -7 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -7 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 1024 } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"111\" frequency: 2200 num_cores: 4 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 57671680 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -7 } dim { size: 1024 } dim { size: 1024 } dim { size: 3 } } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (4, 1024, 1024, 3)\n",
      "Boxes shape: (4, 120, 4)\n",
      "Classes shape: (4, 120)\n"
     ]
    }
   ],
   "source": [
    "for images, bounding_boxes in train_strong_dataset.take(1):\n",
    "    bboxes = bounding_boxes[\"boxes\"]\n",
    "    classes = bounding_boxes[\"classes\"]\n",
    "\n",
    "    print(\"Images shape:\", images.shape)\n",
    "    print(\"Boxes shape:\", bboxes.shape)\n",
    "    print(\"Classes shape:\", classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T15:36:18.218329Z",
     "iopub.status.busy": "2025-10-24T15:36:18.217587Z",
     "iopub.status.idle": "2025-10-24T15:52:55.883984Z",
     "shell.execute_reply": "2025-10-24T15:52:55.881980Z",
     "shell.execute_reply.started": "2025-10-24T15:36:18.218288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_light_dataset: 0/2744 samples have no boxes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1761320565.210547      36 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: -8 } dim { size: 2048 } dim { size: 2048 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -7 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -7 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 1024 } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"111\" frequency: 2200 num_cores: 4 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 57671680 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -7 } dim { size: 1024 } dim { size: 1024 } dim { size: 3 } } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_strong_dataset: 0/2744 samples have no boxes\n",
      "val_dataset: 0/672 samples have no boxes\n"
     ]
    }
   ],
   "source": [
    "def count_empty(ds, name):\n",
    "    empty = 0\n",
    "    total = 0\n",
    "    for images, bboxes in ds.unbatch().take(10000):  # adjust limit if needed\n",
    "        total += 1\n",
    "        if tf.shape(bboxes[\"boxes\"])[0] == 0:\n",
    "            empty += 1\n",
    "    print(f\"{name}: {empty}/{total} samples have no boxes\")\n",
    "\n",
    "count_empty(train_light_dataset, \"train_light_dataset\")\n",
    "count_empty(train_strong_dataset, \"train_strong_dataset\")\n",
    "count_empty(val_dataset, \"val_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T16:52:23.915999Z",
     "iopub.status.busy": "2025-10-21T16:52:23.915294Z",
     "iopub.status.idle": "2025-10-21T16:52:23.920254Z",
     "shell.execute_reply": "2025-10-21T16:52:23.919723Z",
     "shell.execute_reply.started": "2025-10-21T16:52:23.915974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per Epoch: 687\n",
      "Validation Steps: 169\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN_IMAGES = len(train_dicts)\n",
    "NUM_VAL_IMAGES   = len(val_dicts)\n",
    "\n",
    "steps_per_epoch  = math.ceil(NUM_TRAIN_IMAGES / BATCH_SIZE)\n",
    "validation_steps = math.ceil(NUM_VAL_IMAGES / BATCH_SIZE)\n",
    "\n",
    "print(f\"Steps per Epoch: {steps_per_epoch}\")\n",
    "print(f\"Validation Steps: {validation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After creating your datasets...\n",
    "del train_dicts, val_dicts, data_dicts, annonated_ids, all_images, empty_images\n",
    "import gc\n",
    "gc.collect() # Force garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dataset(dataset, rows=2, cols=2, value_range=(0.0, 1.0), bounding_box_format=\"xyxy\"):\n",
    "    # Take a single batch\n",
    "    batch = next(iter(dataset.take(1)))\n",
    "    images, bounding_boxes = batch# our dataset is already (images, bounding_boxes)\n",
    "    \n",
    "    num_images = rows * cols\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols, figsize= (4* cols, 4* rows))\n",
    "    axs = axs.flatten()\n",
    "    for i in range(num_images):\n",
    "        img = images[i].numpy().astype('uint8')\n",
    "\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].set_title('Raw Image')\n",
    "        axs[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "         \n",
    "    # Plot bounding box gallery\n",
    "    keras_cv.visualization.plot_bounding_box_gallery(\n",
    "        images,                 # images\n",
    "        y_pred= bounding_boxes,            # y_true\n",
    "        value_range=value_range,    # range of image values\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        scale=5,\n",
    "        font_scale=0.7,\n",
    "        bounding_box_format=bounding_box_format,\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "visualize_dataset(train_strong_dataset, rows=2, cols=2)\n",
    "visualize_dataset(val_dataset, rows=2, cols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T16:50:37.496902Z",
     "iopub.status.busy": "2025-10-19T16:50:37.496404Z",
     "iopub.status.idle": "2025-10-19T16:50:37.501141Z",
     "shell.execute_reply": "2025-10-19T16:50:37.500279Z",
     "shell.execute_reply.started": "2025-10-19T16:50:37.496878Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    backbone = keras_cv.models.YOLOV8Backbone.from_preset(\n",
    "        'yolo_v8_m_backbone_coco',\n",
    "        name= 'yolov8_backbone'\n",
    "    )\n",
    "\n",
    "    model = keras_cv.models.YOLOV8Detector(\n",
    "        num_classes= NUM_CLASSES,\n",
    "        bounding_box_format= 'xyxy',\n",
    "        fpn_depth= 3,\n",
    "        backbone= backbone,\n",
    "        name= 'yolov8_detector'\n",
    "    )\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T16:50:39.318698Z",
     "iopub.status.busy": "2025-10-19T16:50:39.317980Z",
     "iopub.status.idle": "2025-10-19T16:50:47.744399Z",
     "shell.execute_reply": "2025-10-19T16:50:47.743893Z",
     "shell.execute_reply.started": "2025-10-19T16:50:39.318674Z"
    }
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    \n",
    "    model = create_model()\n",
    "    for layer in model.backbone.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Freeze BN stats explicitly\n",
    "    for layer in model.backbone.layers:\n",
    "        if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            layer.trainable = False\n",
    "\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.AdamW(\n",
    "    learning_rate= WARMUP_LR,\n",
    "    weight_decay= 1e-4,\n",
    "    beta_1= 0.9,\n",
    "    beta_2= 0.999,\n",
    "    global_clipnorm= GLOBAL_CLIPNORM)\n",
    "\n",
    "    classification_loss = keras_cv.losses.FocalLoss()\n",
    "    model.compile(\n",
    "        optimizer= optimizer,\n",
    "        classification_loss= classification_loss,\n",
    "        box_loss= 'ciou',\n",
    "        steps_per_execution= 32 if isinstance(strategy, tf.distribute.TPUStrategy) else 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T12:52:10.625452Z",
     "iopub.status.busy": "2025-10-15T12:52:10.625178Z",
     "iopub.status.idle": "2025-10-15T12:53:11.352226Z",
     "shell.execute_reply": "2025-10-15T12:53:11.351653Z",
     "shell.execute_reply.started": "2025-10-15T12:52:10.625434Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take one batch\n",
    "example_batch = next(iter(train_strong_dataset.take(1)))\n",
    "images, bounding_boxes = example_batch  # unpack tuple\n",
    "\n",
    "# Evaluate\n",
    "try:\n",
    "    model.evaluate(train_strong_dataset.take(1), verbose=True)\n",
    "except Exception as e:\n",
    "    print(\"Your model is not compatible with the dataset you defined earlier.\")\n",
    "    print(\"Error:\", e)\n",
    "else:\n",
    "    # Predict using the images dict\n",
    "    predictions = model.predict(images, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T16:52:30.275889Z",
     "iopub.status.busy": "2025-10-21T16:52:30.275612Z",
     "iopub.status.idle": "2025-10-21T16:52:30.284001Z",
     "shell.execute_reply": "2025-10-21T16:52:30.283427Z",
     "shell.execute_reply.started": "2025-10-21T16:52:30.275867Z"
    }
   },
   "outputs": [],
   "source": [
    "class EvaluateCOCOMetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, data, save_path):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.metrics = keras_cv.metrics.BoxCOCOMetrics(\n",
    "            bounding_box_format=\"xyxy\",\n",
    "            evaluate_freq=1e9,  # We will control evaluation timing manually\n",
    "        )\n",
    "        self.save_path = save_path\n",
    "        self.best_map = -1.0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.metrics.reset_state()\n",
    "\n",
    "        # ---- START: MODIFIED SECTION ----\n",
    "        # 1. Create lists to hold all ground truth and prediction data\n",
    "        y_true_list = []\n",
    "        y_pred_list = []\n",
    "\n",
    "        # 2. Iterate through the entire validation dataset to collect data\n",
    "        for images, y_true in self.data:\n",
    "            y_pred = self.model.predict(images, verbose=0)\n",
    "            y_true_list.append(y_true)\n",
    "            y_pred_list.append(y_pred)\n",
    "\n",
    "        # 3. Concatenate all batches into single, large ragged tensors\n",
    "        y_true_concat = {\n",
    "            'boxes': tf.concat([item['boxes'] for item in y_true_list], axis=0),\n",
    "            'classes': tf.concat([item['classes'] for item in y_true_list], axis=0)\n",
    "        }\n",
    "        # Note: model prediction includes 'confidence', which we also need to concatenate\n",
    "        y_pred_concat = {\n",
    "            'boxes': tf.concat([item['boxes'] for item in y_pred_list], axis=0),\n",
    "            'classes': tf.concat([item['classes'] for item in y_pred_list], axis=0),\n",
    "            'confidence': tf.concat([item['confidence'] for item in y_pred_list], axis=0)\n",
    "        }\n",
    "        # ---- END: MODIFIED SECTION ----\n",
    "\n",
    "        # 4. Update the metric's state ONCE with the full dataset\n",
    "        self.metrics.update_state(y_true_concat, y_pred_concat)\n",
    "\n",
    "        # 5. Get the final results\n",
    "        metrics = self.metrics.result(force=True)\n",
    "        logs.update(metrics)\n",
    "\n",
    "        current_map = metrics[\"MaP\"]\n",
    "        \n",
    "        # Manually print the validation metrics\n",
    "        print(f\"\\nEpoch {epoch+1}: Validation Metrics\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "            \n",
    "        if current_map > self.best_map:\n",
    "            self.best_map = current_map\n",
    "            self.model.save(self.save_path)\n",
    "            print(f\"✅ Validation MaP improved to {current_map:.4f}. Model saved to {self.save_path}\")\n",
    "\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T17:14:06.359266Z",
     "iopub.status.busy": "2025-10-20T17:14:06.358716Z",
     "iopub.status.idle": "2025-10-20T17:14:06.364251Z",
     "shell.execute_reply": "2025-10-20T17:14:06.363420Z",
     "shell.execute_reply.started": "2025-10-20T17:14:06.359239Z"
    }
   },
   "outputs": [],
   "source": [
    "phase1_saved_path = \"/kaggle/working/phase1_best_model.keras\"\n",
    "coco_cb = EvaluateCOCOMetricsCallback(val_dataset, \n",
    "                                      save_path= phase1_saved_path,\n",
    "                                      )\n",
    "early_stopping_cb = EarlyStopping(\n",
    "    monitor= 'MaP',\n",
    "    patience= 3,\n",
    "    restore_best_weights= True,\n",
    "    mode= 'max'\n",
    ")\n",
    "\n",
    "reduce_lr_cb = ReduceLROnPlateau(\n",
    "    monitor= 'MaP',\n",
    "    patience= 3,\n",
    "    factor= 0.66,\n",
    "    min_lr= WARMUP_LR * 0.1,\n",
    "    verbose= 1\n",
    ")\n",
    "\n",
    "tb_cb = TensorBoard(\n",
    "    log_dir= '/kaggle/working/logs',\n",
    "    histogram_freq= 1\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    coco_cb,\n",
    "    early_stopping_cb,\n",
    "    reduce_lr_cb,\n",
    "    tb_cb\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:39:07.163394Z",
     "iopub.status.busy": "2025-10-15T13:39:07.162874Z",
     "iopub.status.idle": "2025-10-15T16:06:21.015672Z",
     "shell.execute_reply": "2025-10-15T16:06:21.015077Z",
     "shell.execute_reply.started": "2025-10-15T13:39:07.163368Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the number of epochs for this training phase\n",
    "print(\"--- Starting Phase 1: Warmup Training ---\")\n",
    "# Fit the model to the training data\n",
    "history = model.fit(train_strong_dataset.repeat(), \n",
    "                    validation_data= val_dataset.repeat(),\n",
    "                    epochs= WARMUP_EPOCH,\n",
    "                    callbacks= [callbacks],\n",
    "                    steps_per_epoch= steps_per_epoch,\n",
    "                    validation_steps= validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T15:57:14.010911Z",
     "iopub.status.busy": "2025-10-21T15:57:14.010369Z",
     "iopub.status.idle": "2025-10-21T15:57:19.264461Z",
     "shell.execute_reply": "2025-10-21T15:57:19.263572Z",
     "shell.execute_reply.started": "2025-10-21T15:57:14.010890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from warmup phase...\n",
      "Model loaded successfully. Ready for Mid-Tune phase !\n",
      "\n",
      "--- Model configured for Phase 2: Mid-Tune ---\n"
     ]
    }
   ],
   "source": [
    "START_UNFREEZE_LAYER_NAME = 'stack4_downsample_conv'\n",
    "with strategy.scope():\n",
    "    print(\"Loading model from warmup phase...\")\n",
    "    model = tf.keras.models.load_model(\n",
    "        '/kaggle/input/wheat-detection/keras/default/1/phase1_best_model.keras',\n",
    "            custom_objects = {\n",
    "                'YOLOV8Detector': keras_cv.models.YOLOV8Detector,\n",
    "                'YOLOV8Backbone': keras_cv.models.YOLOV8Backbone\n",
    "            }\n",
    "    )\n",
    "    print(\"Model loaded successfully. Ready for Mid-Tune phase !\")\n",
    "    \n",
    "    model.backbone.trainable = True\n",
    "    unfreeze_checkpoint = False\n",
    "\n",
    "    for layer in model.backbone.layers:\n",
    "        if layer.name == START_UNFREEZE_LAYER_NAME:\n",
    "            unfreeze_checkpoint = True\n",
    "        if unfreeze_checkpoint:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "\n",
    "        if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            layer.trainable = False\n",
    "\n",
    "    for layer in model.layers: # Iterate through all layers of the detector model\n",
    "    # Note: We re-check for BN to catch those in the Neck and Head\n",
    "        if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            layer.trainable = False\n",
    "    \n",
    "    num_phase2_epochs = INTERMEDIATE_EPOCH - WARMUP_EPOCH\n",
    "    decay_steps = int(steps_per_epoch * num_phase2_epochs)\n",
    "    learning_rate = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=FINE_TUNE_BB_LR,\n",
    "        decay_steps=decay_steps,\n",
    "        alpha=0.1 # End LR will be 10% of initial LR (5e-6)\n",
    "    )\n",
    "\n",
    "    optimizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate = learning_rate,\n",
    "        weight_decay = 1e-4,\n",
    "        beta_1 = 0.9,\n",
    "        beta_2 = 0.999,\n",
    "        global_clipnorm = GLOBAL_CLIPNORM\n",
    "    )\n",
    "\n",
    "    classification_loss = keras_cv.losses.FocalLoss()\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer = optimizer,\n",
    "        classification_loss = classification_loss,\n",
    "        box_loss = 'ciou',\n",
    "        steps_per_execution= 32 if isinstance(strategy, tf.distribute.TPUStrategy) else 1\n",
    "    )\n",
    "    print(\"\\n--- Model configured for Phase 2: Mid-Tune ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T15:57:23.042342Z",
     "iopub.status.busy": "2025-10-21T15:57:23.041725Z",
     "iopub.status.idle": "2025-10-21T15:57:23.046596Z",
     "shell.execute_reply": "2025-10-21T15:57:23.046036Z",
     "shell.execute_reply.started": "2025-10-21T15:57:23.042320Z"
    }
   },
   "outputs": [],
   "source": [
    "phase2_saved_path = \"/kaggle/working/midtune_best_model.keras\"\n",
    "coco_cb = EvaluateCOCOMetricsCallback(val_dataset, \n",
    "                                       phase2_saved_path)\n",
    "early_stopping_cb = EarlyStopping(\n",
    "    monitor= 'MaP',\n",
    "    patience= 5,\n",
    "    restore_best_weights= True,\n",
    "    mode= 'max'\n",
    ")\n",
    "\n",
    "tb_cb = TensorBoard(\n",
    "    log_dir= '/kaggle/working/logs',\n",
    "    histogram_freq= 1\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    coco_cb,\n",
    "    early_stopping_cb,\n",
    "    tb_cb\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T15:57:24.761638Z",
     "iopub.status.busy": "2025-10-21T15:57:24.761076Z",
     "iopub.status.idle": "2025-10-21T16:33:38.767169Z",
     "shell.execute_reply": "2025-10-21T16:33:38.761156Z",
     "shell.execute_reply.started": "2025-10-21T15:57:24.761617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Phase 2: Mid-Tune Training ---\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1761062280.765429      97 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530ms/step - box_loss: 1.1718 - class_loss: 3.0198e-06 - loss: 1.1718\n",
      "Epoch 11: Validation Metrics\n",
      "  MaP: 0.4798\n",
      "  MaP@[IoU=50]: 0.8474\n",
      "  MaP@[IoU=75]: 0.4832\n",
      "  MaP@[area=small]: 0.0848\n",
      "  MaP@[area=medium]: 0.4800\n",
      "  MaP@[area=large]: 0.4954\n",
      "  Recall@[max_detections=1]: 0.0159\n",
      "  Recall@[max_detections=10]: 0.1481\n",
      "  Recall@[max_detections=100]: 0.5838\n",
      "  Recall@[area=small]: 0.2017\n",
      "  Recall@[area=medium]: 0.5836\n",
      "  Recall@[area=large]: 0.6012\n",
      "✅ Validation MaP improved to 0.4798. Model saved to /kaggle/working/midtune_best_model.keras\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m634s\u001b[0m 880ms/step - box_loss: 1.1718 - class_loss: 3.0198e-06 - loss: 1.1718 - val_box_loss: 1.1342 - val_class_loss: 3.0032e-06 - val_loss: 1.1342 - MaP: 0.4798 - MaP@[IoU=50]: 0.8474 - MaP@[IoU=75]: 0.4832 - MaP@[area=small]: 0.0848 - MaP@[area=medium]: 0.4800 - MaP@[area=large]: 0.4954 - Recall@[max_detections=1]: 0.0159 - Recall@[max_detections=10]: 0.1481 - Recall@[max_detections=100]: 0.5838 - Recall@[area=small]: 0.2017 - Recall@[area=medium]: 0.5836 - Recall@[area=large]: 0.6012\n",
      "Epoch 12/30\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514ms/step - box_loss: 1.1376 - class_loss: 2.9990e-06 - loss: 1.1376\n",
      "Epoch 12: Validation Metrics\n",
      "  MaP: 0.4885\n",
      "  MaP@[IoU=50]: 0.8570\n",
      "  MaP@[IoU=75]: 0.4976\n",
      "  MaP@[area=small]: 0.0876\n",
      "  MaP@[area=medium]: 0.4883\n",
      "  MaP@[area=large]: 0.5091\n",
      "  Recall@[max_detections=1]: 0.0161\n",
      "  Recall@[max_detections=10]: 0.1494\n",
      "  Recall@[max_detections=100]: 0.5861\n",
      "  Recall@[area=small]: 0.1971\n",
      "  Recall@[area=medium]: 0.5820\n",
      "  Recall@[area=large]: 0.6192\n",
      "✅ Validation MaP improved to 0.4885. Model saved to /kaggle/working/midtune_best_model.keras\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m578s\u001b[0m 842ms/step - box_loss: 1.1376 - class_loss: 2.9989e-06 - loss: 1.1376 - val_box_loss: 1.1323 - val_class_loss: 2.9902e-06 - val_loss: 1.1323 - MaP: 0.4885 - MaP@[IoU=50]: 0.8570 - MaP@[IoU=75]: 0.4976 - MaP@[area=small]: 0.0876 - MaP@[area=medium]: 0.4883 - MaP@[area=large]: 0.5091 - Recall@[max_detections=1]: 0.0161 - Recall@[max_detections=10]: 0.1494 - Recall@[max_detections=100]: 0.5861 - Recall@[area=small]: 0.1971 - Recall@[area=medium]: 0.5820 - Recall@[area=large]: 0.6192\n",
      "Epoch 13/30\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514ms/step - box_loss: 1.1274 - class_loss: 2.9910e-06 - loss: 1.1274\n",
      "Epoch 13: Validation Metrics\n",
      "  MaP: 0.4889\n",
      "  MaP@[IoU=50]: 0.8584\n",
      "  MaP@[IoU=75]: 0.4966\n",
      "  MaP@[area=small]: 0.0761\n",
      "  MaP@[area=medium]: 0.4883\n",
      "  MaP@[area=large]: 0.5092\n",
      "  Recall@[max_detections=1]: 0.0161\n",
      "  Recall@[max_detections=10]: 0.1499\n",
      "  Recall@[max_detections=100]: 0.5894\n",
      "  Recall@[area=small]: 0.2083\n",
      "  Recall@[area=medium]: 0.5880\n",
      "  Recall@[area=large]: 0.6118\n",
      "✅ Validation MaP improved to 0.4889. Model saved to /kaggle/working/midtune_best_model.keras\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m581s\u001b[0m 846ms/step - box_loss: 1.1274 - class_loss: 2.9910e-06 - loss: 1.1274 - val_box_loss: 1.1305 - val_class_loss: 2.9679e-06 - val_loss: 1.1305 - MaP: 0.4889 - MaP@[IoU=50]: 0.8584 - MaP@[IoU=75]: 0.4966 - MaP@[area=small]: 0.0761 - MaP@[area=medium]: 0.4883 - MaP@[area=large]: 0.5092 - Recall@[max_detections=1]: 0.0161 - Recall@[max_detections=10]: 0.1499 - Recall@[max_detections=100]: 0.5894 - Recall@[area=small]: 0.2083 - Recall@[area=medium]: 0.5880 - Recall@[area=large]: 0.6118\n",
      "Epoch 14/30\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508ms/step - box_loss: 1.1117 - class_loss: 2.9609e-06 - loss: 1.1117"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node yolov8_label_encoder_1/cond/mul_5 defined at (most recent call last):\n  File \"/usr/lib/python3.11/threading.py\", line 1002, in _bootstrap\n\n  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_cv/src/models/object_detection/yolo_v8/yolo_v8_detector.py\", line 532, in test_step\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 92, in test_step\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/trainer.py\", line 387, in _compute_loss\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_cv/src/models/object_detection/yolo_v8/yolo_v8_detector.py\", line 555, in compute_loss\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 908, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_cv/src/models/object_detection/yolo_v8/yolo_v8_label_encoder.py\", line 248, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/core.py\", line 1033, in cond\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/core.py\", line 987, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/core.py\", line 979, in call_fn\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/core.py\", line 993, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/core.py\", line 240, in cond\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_cv/src/models/object_detection/yolo_v8/yolo_v8_label_encoder.py\", line 250, in <lambda>\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_cv/src/models/object_detection/yolo_v8/yolo_v8_label_encoder.py\", line 111, in assign\n\nrequired broadcastable shapes\n\t [[{{node yolov8_label_encoder_1/cond/mul_5}}]] [Op:__inference_multi_step_on_iterator_75803]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/1912659896.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- Starting Phase 2: Mid-Tune Training ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m final_history = model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_light_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mINTERMEDIATE_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mWARMUP_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node yolov8_label_encoder_1/cond/mul_5 defined at (most recent call last):\n  File \"/usr/lib/python3.11/threading.py\", line 1002, in _bootstrap\n\n  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_cv/src/models/object_detection/yolo_v8/yolo_v8_detector.py\", line 532, in test_step\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 92, in test_step\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/trainer.py\", line 387, in _compute_loss\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_cv/src/models/object_detection/yolo_v8/yolo_v8_detector.py\", line 555, in compute_loss\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 908, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_cv/src/models/object_detection/yolo_v8/yolo_v8_label_encoder.py\", line 248, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/core.py\", line 1033, in cond\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/core.py\", line 987, in __call__\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/core.py\", line 979, in call_fn\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/core.py\", line 993, in call\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/core.py\", line 240, in cond\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_cv/src/models/object_detection/yolo_v8/yolo_v8_label_encoder.py\", line 250, in <lambda>\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_cv/src/models/object_detection/yolo_v8/yolo_v8_label_encoder.py\", line 111, in assign\n\nrequired broadcastable shapes\n\t [[{{node yolov8_label_encoder_1/cond/mul_5}}]] [Op:__inference_multi_step_on_iterator_75803]"
     ]
    }
   ],
   "source": [
    "print(\"--- Starting Phase 2: Mid-Tune Training ---\")\n",
    "final_history = model.fit(\n",
    "    train_light_dataset.repeat(),\n",
    "    epochs= INTERMEDIATE_EPOCH,\n",
    "    initial_epoch= WARMUP_EPOCH,\n",
    "    validation_data= val_dataset.repeat(),\n",
    "    steps_per_epoch= steps_per_epoch,\n",
    "    validation_steps= validation_steps,\n",
    "    callbacks= callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T16:58:18.827629Z",
     "iopub.status.busy": "2025-10-21T16:58:18.827366Z",
     "iopub.status.idle": "2025-10-21T16:58:23.302836Z",
     "shell.execute_reply": "2025-10-21T16:58:23.302245Z",
     "shell.execute_reply.started": "2025-10-21T16:58:18.827612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from mid-tune phase...\n",
      "Model loaded successfully. Ready for Fine-Tune phase !\n",
      "\n",
      "--- Model configured for Phase 3: Fine-Tune ---\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    print(\"Loading model from mid-tune phase...\")\n",
    "    model = tf.keras.models.load_model(\n",
    "        '/kaggle/input/wheat-detection/keras/default/2/midtune_best_model.keras',\n",
    "            custom_objects = {\n",
    "                'YOLOV8Detector': keras_cv.models.YOLOV8Detector,\n",
    "                'YOLOV8Backbone': keras_cv.models.YOLOV8Backbone\n",
    "            }\n",
    "    )\n",
    "    print(\"Model loaded successfully. Ready for Fine-Tune phase !\")\n",
    "    \n",
    "    model.backbone.trainable = True\n",
    "\n",
    "    for layer in model.backbone.layers:\n",
    "        if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            layer.trainable = False\n",
    "\n",
    "    for layer in model.layers: # Iterate through all layers of the detector model\n",
    "    # Note: We re-check for BN to catch those in the Neck and Head\n",
    "        if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            layer.trainable = False\n",
    "    \n",
    "    num_phase2_epochs = FINAL_EPOCH - INTERMEDIATE_EPOCH\n",
    "    decay_steps = int(steps_per_epoch * num_phase2_epochs)\n",
    "    learning_rate = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=FINE_TUNE_MODEL_LR,\n",
    "        decay_steps=decay_steps,\n",
    "        alpha=0.1 # End LR will be 10% of initial LR (5e-6)\n",
    "    )\n",
    "\n",
    "    optimizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate = learning_rate,\n",
    "        weight_decay = 1e-4,\n",
    "        beta_1 = 0.9,\n",
    "        beta_2 = 0.999,\n",
    "        global_clipnorm = GLOBAL_CLIPNORM\n",
    "    )\n",
    "\n",
    "    classification_loss = keras_cv.losses.FocalLoss()\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer = optimizer,\n",
    "        classification_loss = classification_loss,\n",
    "        box_loss = 'ciou',\n",
    "        steps_per_execution= 32 if isinstance(strategy, tf.distribute.TPUStrategy) else 1\n",
    "    )\n",
    "    print(\"\\n--- Model configured for Phase 3: Fine-Tune ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T16:59:06.902946Z",
     "iopub.status.busy": "2025-10-21T16:59:06.902366Z",
     "iopub.status.idle": "2025-10-21T16:59:06.907180Z",
     "shell.execute_reply": "2025-10-21T16:59:06.906399Z",
     "shell.execute_reply.started": "2025-10-21T16:59:06.902925Z"
    }
   },
   "outputs": [],
   "source": [
    "phase3_saved_path = \"/kaggle/working/best_model.keras\"\n",
    "coco_cb = EvaluateCOCOMetricsCallback(val_dataset, \n",
    "                                       phase3_saved_path)\n",
    "early_stopping_cb = EarlyStopping(\n",
    "    monitor= 'MaP',\n",
    "    patience= 8,\n",
    "    restore_best_weights= True,\n",
    "    mode= 'max'\n",
    ")\n",
    "\n",
    "tb_cb = TensorBoard(\n",
    "    log_dir= '/kaggle/working/logs',\n",
    "    histogram_freq= 1\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    coco_cb,\n",
    "    early_stopping_cb,\n",
    "    tb_cb\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T16:59:43.333912Z",
     "iopub.status.busy": "2025-10-21T16:59:43.333577Z",
     "iopub.status.idle": "2025-10-21T18:18:18.464676Z",
     "shell.execute_reply": "2025-10-21T18:18:18.463527Z",
     "shell.execute_reply.started": "2025-10-21T16:59:43.333884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Phase 3: Fine-Tune Training ---\n",
      "Epoch 31/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1761066018.639254      95 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583ms/step - box_loss: 1.0839 - class_loss: 2.9265e-06 - loss: 1.0839\n",
      "Epoch 31: Validation Metrics\n",
      "  MaP: 0.4957\n",
      "  MaP@[IoU=50]: 0.8621\n",
      "  MaP@[IoU=75]: 0.5055\n",
      "  MaP@[area=small]: 0.0821\n",
      "  MaP@[area=medium]: 0.4943\n",
      "  MaP@[area=large]: 0.5206\n",
      "  Recall@[max_detections=1]: 0.0163\n",
      "  Recall@[max_detections=10]: 0.1511\n",
      "  Recall@[max_detections=100]: 0.5951\n",
      "  Recall@[area=small]: 0.2198\n",
      "  Recall@[area=medium]: 0.5923\n",
      "  Recall@[area=large]: 0.6224\n",
      "✅ Validation MaP improved to 0.4957. Model saved to /kaggle/working/best_model.keras\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m675s\u001b[0m 933ms/step - box_loss: 1.0838 - class_loss: 2.9265e-06 - loss: 1.0838 - val_box_loss: 1.1213 - val_class_loss: 2.9558e-06 - val_loss: 1.1213 - MaP: 0.4957 - MaP@[IoU=50]: 0.8621 - MaP@[IoU=75]: 0.5055 - MaP@[area=small]: 0.0821 - MaP@[area=medium]: 0.4943 - MaP@[area=large]: 0.5206 - Recall@[max_detections=1]: 0.0163 - Recall@[max_detections=10]: 0.1511 - Recall@[max_detections=100]: 0.5951 - Recall@[area=small]: 0.2198 - Recall@[area=medium]: 0.5923 - Recall@[area=large]: 0.6224\n",
      "Epoch 32/80\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563ms/step - box_loss: 1.0673 - class_loss: 2.9073e-06 - loss: 1.0673\n",
      "Epoch 32: Validation Metrics\n",
      "  MaP: 0.4977\n",
      "  MaP@[IoU=50]: 0.8626\n",
      "  MaP@[IoU=75]: 0.5123\n",
      "  MaP@[area=small]: 0.0831\n",
      "  MaP@[area=medium]: 0.4962\n",
      "  MaP@[area=large]: 0.5233\n",
      "  Recall@[max_detections=1]: 0.0165\n",
      "  Recall@[max_detections=10]: 0.1521\n",
      "  Recall@[max_detections=100]: 0.5942\n",
      "  Recall@[area=small]: 0.2318\n",
      "  Recall@[area=medium]: 0.5920\n",
      "  Recall@[area=large]: 0.6190\n",
      "✅ Validation MaP improved to 0.4977. Model saved to /kaggle/working/best_model.keras\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m617s\u001b[0m 898ms/step - box_loss: 1.0673 - class_loss: 2.9073e-06 - loss: 1.0673 - val_box_loss: 1.1049 - val_class_loss: 2.9686e-06 - val_loss: 1.1049 - MaP: 0.4977 - MaP@[IoU=50]: 0.8626 - MaP@[IoU=75]: 0.5123 - MaP@[area=small]: 0.0831 - MaP@[area=medium]: 0.4962 - MaP@[area=large]: 0.5233 - Recall@[max_detections=1]: 0.0165 - Recall@[max_detections=10]: 0.1521 - Recall@[max_detections=100]: 0.5942 - Recall@[area=small]: 0.2318 - Recall@[area=medium]: 0.5920 - Recall@[area=large]: 0.6190\n",
      "Epoch 33/80\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559ms/step - box_loss: 1.0576 - class_loss: 2.8872e-06 - loss: 1.0576\n",
      "Epoch 33: Validation Metrics\n",
      "  MaP: 0.5012\n",
      "  MaP@[IoU=50]: 0.8657\n",
      "  MaP@[IoU=75]: 0.5158\n",
      "  MaP@[area=small]: 0.0840\n",
      "  MaP@[area=medium]: 0.4990\n",
      "  MaP@[area=large]: 0.5310\n",
      "  Recall@[max_detections=1]: 0.0164\n",
      "  Recall@[max_detections=10]: 0.1514\n",
      "  Recall@[max_detections=100]: 0.5990\n",
      "  Recall@[area=small]: 0.2269\n",
      "  Recall@[area=medium]: 0.5952\n",
      "  Recall@[area=large]: 0.6305\n",
      "✅ Validation MaP improved to 0.5012. Model saved to /kaggle/working/best_model.keras\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 892ms/step - box_loss: 1.0576 - class_loss: 2.8872e-06 - loss: 1.0576 - val_box_loss: 1.1065 - val_class_loss: 2.9384e-06 - val_loss: 1.1065 - MaP: 0.5012 - MaP@[IoU=50]: 0.8657 - MaP@[IoU=75]: 0.5158 - MaP@[area=small]: 0.0840 - MaP@[area=medium]: 0.4990 - MaP@[area=large]: 0.5310 - Recall@[max_detections=1]: 0.0164 - Recall@[max_detections=10]: 0.1514 - Recall@[max_detections=100]: 0.5990 - Recall@[area=small]: 0.2269 - Recall@[area=medium]: 0.5952 - Recall@[area=large]: 0.6305\n",
      "Epoch 34/80\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554ms/step - box_loss: 1.0592 - class_loss: 2.8942e-06 - loss: 1.0592\n",
      "Epoch 34: Validation Metrics\n",
      "  MaP: 0.5024\n",
      "  MaP@[IoU=50]: 0.8648\n",
      "  MaP@[IoU=75]: 0.5175\n",
      "  MaP@[area=small]: 0.0912\n",
      "  MaP@[area=medium]: 0.5015\n",
      "  MaP@[area=large]: 0.5300\n",
      "  Recall@[max_detections=1]: 0.0163\n",
      "  Recall@[max_detections=10]: 0.1520\n",
      "  Recall@[max_detections=100]: 0.5990\n",
      "  Recall@[area=small]: 0.2223\n",
      "  Recall@[area=medium]: 0.5966\n",
      "  Recall@[area=large]: 0.6247\n",
      "✅ Validation MaP improved to 0.5024. Model saved to /kaggle/working/best_model.keras\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 890ms/step - box_loss: 1.0592 - class_loss: 2.8942e-06 - loss: 1.0592 - val_box_loss: 1.1032 - val_class_loss: 2.9360e-06 - val_loss: 1.1032 - MaP: 0.5024 - MaP@[IoU=50]: 0.8648 - MaP@[IoU=75]: 0.5175 - MaP@[area=small]: 0.0912 - MaP@[area=medium]: 0.5015 - MaP@[area=large]: 0.5300 - Recall@[max_detections=1]: 0.0163 - Recall@[max_detections=10]: 0.1520 - Recall@[max_detections=100]: 0.5990 - Recall@[area=small]: 0.2223 - Recall@[area=medium]: 0.5966 - Recall@[area=large]: 0.6247\n",
      "Epoch 35/80\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540ms/step - box_loss: 1.0474 - class_loss: 2.8857e-06 - loss: 1.0474\n",
      "Epoch 35: Validation Metrics\n",
      "  MaP: 0.4993\n",
      "  MaP@[IoU=50]: 0.8627\n",
      "  MaP@[IoU=75]: 0.5134\n",
      "  MaP@[area=small]: 0.0919\n",
      "  MaP@[area=medium]: 0.4965\n",
      "  MaP@[area=large]: 0.5291\n",
      "  Recall@[max_detections=1]: 0.0164\n",
      "  Recall@[max_detections=10]: 0.1517\n",
      "  Recall@[max_detections=100]: 0.5987\n",
      "  Recall@[area=small]: 0.2289\n",
      "  Recall@[area=medium]: 0.5957\n",
      "  Recall@[area=large]: 0.6269\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m617s\u001b[0m 899ms/step - box_loss: 1.0474 - class_loss: 2.8857e-06 - loss: 1.0474 - val_box_loss: 1.1045 - val_class_loss: 2.9371e-06 - val_loss: 1.1045 - MaP: 0.4993 - MaP@[IoU=50]: 0.8627 - MaP@[IoU=75]: 0.5134 - MaP@[area=small]: 0.0919 - MaP@[area=medium]: 0.4965 - MaP@[area=large]: 0.5291 - Recall@[max_detections=1]: 0.0164 - Recall@[max_detections=10]: 0.1517 - Recall@[max_detections=100]: 0.5987 - Recall@[area=small]: 0.2289 - Recall@[area=medium]: 0.5957 - Recall@[area=large]: 0.6269\n",
      "Epoch 36/80\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529ms/step - box_loss: 1.0397 - class_loss: 2.8758e-06 - loss: 1.0397\n",
      "Epoch 36: Validation Metrics\n",
      "  MaP: 0.5028\n",
      "  MaP@[IoU=50]: 0.8624\n",
      "  MaP@[IoU=75]: 0.5167\n",
      "  MaP@[area=small]: 0.0961\n",
      "  MaP@[area=medium]: 0.5004\n",
      "  MaP@[area=large]: 0.5271\n",
      "  Recall@[max_detections=1]: 0.0163\n",
      "  Recall@[max_detections=10]: 0.1522\n",
      "  Recall@[max_detections=100]: 0.5958\n",
      "  Recall@[area=small]: 0.2161\n",
      "  Recall@[area=medium]: 0.5941\n",
      "  Recall@[area=large]: 0.6194\n",
      "✅ Validation MaP improved to 0.5028. Model saved to /kaggle/working/best_model.keras\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m603s\u001b[0m 878ms/step - box_loss: 1.0397 - class_loss: 2.8758e-06 - loss: 1.0397 - val_box_loss: 1.1020 - val_class_loss: 2.9535e-06 - val_loss: 1.1020 - MaP: 0.5028 - MaP@[IoU=50]: 0.8624 - MaP@[IoU=75]: 0.5167 - MaP@[area=small]: 0.0961 - MaP@[area=medium]: 0.5004 - MaP@[area=large]: 0.5271 - Recall@[max_detections=1]: 0.0163 - Recall@[max_detections=10]: 0.1522 - Recall@[max_detections=100]: 0.5958 - Recall@[area=small]: 0.2161 - Recall@[area=medium]: 0.5941 - Recall@[area=large]: 0.6194\n",
      "Epoch 37/80\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529ms/step - box_loss: 1.0398 - class_loss: 2.8864e-06 - loss: 1.0398\n",
      "Epoch 37: Validation Metrics\n",
      "  MaP: 0.5027\n",
      "  MaP@[IoU=50]: 0.8629\n",
      "  MaP@[IoU=75]: 0.5174\n",
      "  MaP@[area=small]: 0.0862\n",
      "  MaP@[area=medium]: 0.4993\n",
      "  MaP@[area=large]: 0.5288\n",
      "  Recall@[max_detections=1]: 0.0163\n",
      "  Recall@[max_detections=10]: 0.1526\n",
      "  Recall@[max_detections=100]: 0.5948\n",
      "  Recall@[area=small]: 0.2236\n",
      "  Recall@[area=medium]: 0.5915\n",
      "  Recall@[area=large]: 0.6244\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m601s\u001b[0m 875ms/step - box_loss: 1.0398 - class_loss: 2.8864e-06 - loss: 1.0398 - val_box_loss: 1.1048 - val_class_loss: 2.9492e-06 - val_loss: 1.1048 - MaP: 0.5027 - MaP@[IoU=50]: 0.8629 - MaP@[IoU=75]: 0.5174 - MaP@[area=small]: 0.0862 - MaP@[area=medium]: 0.4993 - MaP@[area=large]: 0.5288 - Recall@[max_detections=1]: 0.0163 - Recall@[max_detections=10]: 0.1526 - Recall@[max_detections=100]: 0.5948 - Recall@[area=small]: 0.2236 - Recall@[area=medium]: 0.5915 - Recall@[area=large]: 0.6244\n",
      "Epoch 38/80\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527ms/step - box_loss: 1.0324 - class_loss: 2.8586e-06 - loss: 1.0324"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node jittered_resize_1_1/SelectV2_4 defined at (most recent call last):\n<stack traces unavailable>\nDetected at node jittered_resize_1_1/SelectV2_4 defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Error in user-defined function passed to ParallelMapDatasetV2:14 transformation with iterator: Iterator::Root::Prefetch::ForeverRepeat[0]::Prefetch::ParallelMapV2::MapAndBatch::ParallelMapV2:  condition [1,41], then [1,96], and else [] must be broadcastable\n\t [[{{node jittered_resize_1_1/SelectV2_4}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNext]]\n\t [[StatefulPartitionedCall/yolov8_label_encoder_1/cond/pivot_t/_436/_33]]\n  (1) INVALID_ARGUMENT:  Error in user-defined function passed to ParallelMapDatasetV2:14 transformation with iterator: Iterator::Root::Prefetch::ForeverRepeat[0]::Prefetch::ParallelMapV2::MapAndBatch::ParallelMapV2:  condition [1,41], then [1,96], and else [] must be broadcastable\n\t [[{{node jittered_resize_1_1/SelectV2_4}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_multi_step_on_iterator_82178]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/3959577544.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- Starting Phase 3: Fine-Tune Training ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m final_history = model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_light_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mFINAL_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mINTERMEDIATE_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node jittered_resize_1_1/SelectV2_4 defined at (most recent call last):\n<stack traces unavailable>\nDetected at node jittered_resize_1_1/SelectV2_4 defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Error in user-defined function passed to ParallelMapDatasetV2:14 transformation with iterator: Iterator::Root::Prefetch::ForeverRepeat[0]::Prefetch::ParallelMapV2::MapAndBatch::ParallelMapV2:  condition [1,41], then [1,96], and else [] must be broadcastable\n\t [[{{node jittered_resize_1_1/SelectV2_4}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNext]]\n\t [[StatefulPartitionedCall/yolov8_label_encoder_1/cond/pivot_t/_436/_33]]\n  (1) INVALID_ARGUMENT:  Error in user-defined function passed to ParallelMapDatasetV2:14 transformation with iterator: Iterator::Root::Prefetch::ForeverRepeat[0]::Prefetch::ParallelMapV2::MapAndBatch::ParallelMapV2:  condition [1,41], then [1,96], and else [] must be broadcastable\n\t [[{{node jittered_resize_1_1/SelectV2_4}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_multi_step_on_iterator_82178]"
     ]
    }
   ],
   "source": [
    "print(\"--- Starting Phase 3: Fine-Tune Training ---\")\n",
    "final_history = model.fit(\n",
    "    train_light_dataset.repeat(),\n",
    "    epochs= FINAL_EPOCH,\n",
    "    initial_epoch= INTERMEDIATE_EPOCH,\n",
    "    validation_data= val_dataset.repeat(),\n",
    "    steps_per_epoch= steps_per_epoch,\n",
    "    validation_steps= validation_steps,\n",
    "    callbacks= callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections(model, dataset, bounding_box_format):\n",
    "    images, y_true = next(iter(dataset.take(1)))\n",
    "    y_pred = model.predict(images)\n",
    "\n",
    "    # y_pred is already in dict format (boxes, classes, confidence)\n",
    "    keras_cv.visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=(0, 255),\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,   # no need for to_ragged\n",
    "        scale=4,\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "        show=True,\n",
    "        font_scale=0.7,\n",
    "    )\n",
    "visualize_detections(yolo_model, val_dataset, bounding_box_format= 'xyxy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T19:49:32.231762Z",
     "iopub.status.busy": "2025-09-27T19:49:32.231496Z",
     "iopub.status.idle": "2025-09-27T19:49:32.236428Z",
     "shell.execute_reply": "2025-09-27T19:49:32.235360Z",
     "shell.execute_reply.started": "2025-09-27T19:49:32.231743Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T20:16:21.541965Z",
     "iopub.status.busy": "2025-09-27T20:16:21.541756Z",
     "iopub.status.idle": "2025-09-27T20:16:29.745683Z",
     "shell.execute_reply": "2025-09-27T20:16:29.744879Z",
     "shell.execute_reply.started": "2025-09-27T20:16:21.541932Z"
    }
   },
   "outputs": [],
   "source": [
    "example_batch = next(iter(train_dataset.take(1)))\n",
    "img, bb = example_batch\n",
    "# Run with model.predict(), not just model()\n",
    "preds = yolo_model.predict(img)\n",
    "\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-28T08:55:02.010Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_for_inference(image_path):\n",
    "    \"\"\"Loads and resizes a single image for model prediction.\"\"\"\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, IMG_SIZE)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-28T08:55:02.008Z"
    }
   },
   "outputs": [],
   "source": [
    "test_image_paths = [os.path.join(TEST_DIR, fname) for fname in os.listdir(TEST_DIR)]\n",
    "\n",
    "# Create a dataset from the file paths\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(test_image_paths)\n",
    "\n",
    "# Map the preprocessing function\n",
    "test_ds = test_ds.map(preprocess_for_inference, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Batch the dataset\n",
    "BATCH_SIZE = 4 # You can adjust this based on your RAM\n",
    "test_ds = test_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Run prediction on the entire test set\n",
    "y_preds = yolo_model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-28T08:55:02.010Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_predictions(image_paths, predictions, count=4, confidence_threshold=0.5):\n",
    "    \"\"\"Visualizes model predictions on a set of images.\"\"\"\n",
    "    num_images_to_show = min(count, len(image_paths))\n",
    "    \n",
    "    # Load the original images for display\n",
    "    images_to_plot = [np.array(Image.open(p)) for p in image_paths[:num_images_to_show]]\n",
    "    \n",
    "    # Extract predictions for the images we'll show\n",
    "    boxes = predictions['boxes'][:num_images_to_show]\n",
    "    confidences = predictions['confidence'][:num_images_to_show]\n",
    "    num_detections = predictions['num_detections'][:num_images_to_show]\n",
    "    \n",
    "    # Create a bounding box dictionary suitable for KerasCV's plot function\n",
    "    y_pred_for_plot = {\n",
    "        'boxes': [],\n",
    "        'classes': [],\n",
    "        'confidence': []\n",
    "    }\n",
    "\n",
    "    for i in range(num_images_to_show):\n",
    "        num_valid = num_detections[i]\n",
    "        \n",
    "        # Filter out padded boxes and low-confidence boxes\n",
    "        valid_indices = confidences[i, :num_valid] >= confidence_threshold\n",
    "        \n",
    "        y_pred_for_plot['boxes'].append(boxes[i, :num_valid][valid_indices])\n",
    "        y_pred_for_plot['classes'].append(np.zeros(np.sum(valid_indices), dtype=int)) # All class 0\n",
    "        y_pred_for_plot['confidence'].append(confidences[i, :num_valid][valid_indices])\n",
    "\n",
    "    # Convert lists to ragged tensors for plotting\n",
    "    y_pred_for_plot['boxes'] = tf.ragged.constant(y_pred_for_plot['boxes'])\n",
    "    y_pred_for_plot['classes'] = tf.ragged.constant(y_pred_for_plot['classes'])\n",
    "    y_pred_for_plot['confidence'] = tf.ragged.constant(y_pred_for_plot['confidence'])\n",
    "    \n",
    "    # Create preprocessed images for correct box scaling\n",
    "    preprocessed_images = [preprocess_for_inference(p) for p in image_paths[:num_images_to_show]]\n",
    "    preprocessed_images = tf.stack(preprocessed_images)\n",
    "\n",
    "    keras_cv.visualization.plot_bounding_box_gallery(\n",
    "        preprocessed_images,\n",
    "        value_range=(0, 255),\n",
    "        bounding_box_format=\"xyxy\",\n",
    "        y_pred=y_pred_for_plot,\n",
    "        scale=4,\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "        font_scale=0.7\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "# Visualize predictions on the first few test images\n",
    "visualize_predictions(test_image_paths, y_preds, count=4, confidence_threshold=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T07:55:52.978944Z",
     "iopub.status.busy": "2025-10-14T07:55:52.978660Z",
     "iopub.status.idle": "2025-10-14T07:55:52.985287Z",
     "shell.execute_reply": "2025-10-14T07:55:52.984110Z",
     "shell.execute_reply.started": "2025-10-14T07:55:52.978925Z"
    }
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "count += sum(1 for layer in model.backbone.layers)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 1160143,
     "sourceId": 19989,
     "sourceType": "competition"
    },
    {
     "modelId": 2804,
     "modelInstanceId": 4650,
     "sourceId": 6107,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 475964,
     "modelInstanceId": 460151,
     "sourceId": 612509,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 475964,
     "modelInstanceId": 460151,
     "sourceId": 615506,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
