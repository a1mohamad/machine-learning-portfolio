{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13836,"databundleVersionId":1718836,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":525409,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":412180,"modelId":429965}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## üöÄ Inference Notebook Strategy for Kaggle Competitions\n\nFor Kaggle competitions that involve models with long training times, like deep learning models for image classification, the best practice is to separate your workflow into two distinct notebooks:\n\n---\n\n### 1. **Training Notebook**\n\nThis notebook is where you'll do all the heavy lifting of training your model.  \n[Cassava Leaf train using EfficientNetB3 fine tuning](https://www.kaggle.com/code/amirmohamadaskari/cassava-leaf-train-efficientnetb3-fine-tuning/edit)\n\n* It should leverage powerful accelerators like **TPUs** or **GPUs** to speed up the training process.\n* Once your model is trained, you'll **save the trained model file** (e.g., as a `.keras` or `SavedModel` format) to your notebook's output directory.\n* After the training is complete and the model is saved, you then **save a version** of this notebook. The output of this notebook (your trained model) becomes a permanent asset on Kaggle.\n\n---\n\n### 2. **Inference Notebook (This Notebook)**\n\nThis notebook is designed specifically for making predictions and generating your submission file. It's built to be lightweight and fast.\n\n* It **doesn't perform any training**. Instead, it **loads the pre-trained model** that you saved as an output from your Training Notebook. To do this, you'll add the Training Notebook's output as an **input dataset** to this Inference Notebook.\n* It then focuses solely on **loading the test data**, using the loaded model to make predictions, and **formatting these predictions into the required `submission.csv` file**.\n* This separation ensures that your submission run is quick and reliable, as it doesn't need to re-train the model every time. Kaggle's submission system will run this entire notebook to generate your final submission.","metadata":{}},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model\nimport numpy as np\nimport pandas as pd\nimport random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T18:18:29.095771Z","iopub.execute_input":"2025-08-19T18:18:29.096047Z","iopub.status.idle":"2025-08-19T18:18:42.897050Z","shell.execute_reply.started":"2025-08-19T18:18:29.096024Z","shell.execute_reply":"2025-08-19T18:18:42.896237Z"}},"outputs":[{"name":"stderr","text":"2025-08-19 18:18:30.548048: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755627510.711915      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755627510.763965      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Some configuration based on training\nAUTO = tf.data.AUTOTUNE\nIMAGE_SIZE = (512, 512)\nBATCH_SIZE_PER_REPLICA = 8\nNUM_CLASSES = 5\nBATCH_SIZE = 8","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T18:19:34.600358Z","iopub.execute_input":"2025-08-19T18:19:34.600626Z","iopub.status.idle":"2025-08-19T18:19:34.604844Z","shell.execute_reply.started":"2025-08-19T18:19:34.600606Z","shell.execute_reply":"2025-08-19T18:19:34.604164Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"These two lines define **key file paths** for your Kaggle notebook:\n\n---\n\n* **`DATA_DIR = '/kaggle/input/cassava-leaf-disease-classification'`**\n    This points to the **competition's dataset**, which Kaggle automatically makes available in the `/kaggle/input/` directory. It's where your notebook finds the images and other data it needs.\n\n---\n\n* **`MODEL_DIR = '/kaggle/input/cassava-leaf-model/tensorflow2/default/1/final_model_cassava.keras'`**\n    This specifies the location of your **trained model file**. On Kaggle, if you train a model in one notebook and want to use it in another (like an inference notebook), you save the training notebook, and its output (your model) can then be added as an **input dataset** to your current notebook. This makes the model accessible via the `/kaggle/input/` path.","metadata":{}},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/cassava-leaf-disease-classification'\nMODEL_DIR = '/kaggle/input/cassava-leaf-model/tensorflow2/default/1/final_model_cassava.keras'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T18:18:50.323055Z","iopub.execute_input":"2025-08-19T18:18:50.323596Z","iopub.status.idle":"2025-08-19T18:18:50.327112Z","shell.execute_reply.started":"2025-08-19T18:18:50.323573Z","shell.execute_reply":"2025-08-19T18:18:50.326263Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Loading model\n\nLet's load our model.  \nI saved my model from training script. \n**Note**: Always save models is good approach cause somehow we can't import output of older versions of our notebook as input. so I uploaded there and use it for inference but main approach that I explained doesn't change at all:      \n[Cassava Leaf Model](https://www.kaggle.com/models/amirmohamadaskari/cassava-leaf-model)","metadata":{}},{"cell_type":"code","source":"model = load_model(MODEL_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T18:18:53.348700Z","iopub.execute_input":"2025-08-19T18:18:53.349228Z","iopub.status.idle":"2025-08-19T18:19:00.002752Z","shell.execute_reply.started":"2025-08-19T18:18:53.349203Z","shell.execute_reply":"2025-08-19T18:19:00.002174Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1755627534.038925      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## üîç Inference on Test Set\n\nWith our model trained and fine-tuned, it's time to generate predictions on the unseen test data. We'll first create a data pipeline for the test set, similar to the one we built for training and validation.","metadata":{}},{"cell_type":"code","source":"# This function decodes examples from the test TFRecord files.\n# Note that test files contain an 'image_name' instead of a 'target' label.\ndef decode_test_example(example):\n    feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'image_name': tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, feature_description)\n    \n    # Decode and process the image\n    image = tf.image.decode_jpeg(example['image'], channels=3)\n    image = tf.image.resize(image, IMAGE_SIZE)\n    \n    # Return the image and its ID\n    return image, example['image_name']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T18:19:06.989395Z","iopub.execute_input":"2025-08-19T18:19:06.989936Z","iopub.status.idle":"2025-08-19T18:19:06.994676Z","shell.execute_reply.started":"2025-08-19T18:19:06.989915Z","shell.execute_reply":"2025-08-19T18:19:06.993917Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from tensorflow.keras.applications.efficientnet import preprocess_input\n\ndef preprocess(image, label):\n    # Apply the specific preprocessing required by the EfficientNet model\n    image = preprocess_input(image)\n    return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T18:19:07.255420Z","iopub.execute_input":"2025-08-19T18:19:07.255969Z","iopub.status.idle":"2025-08-19T18:19:07.263568Z","shell.execute_reply.started":"2025-08-19T18:19:07.255947Z","shell.execute_reply":"2025-08-19T18:19:07.262820Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Test-Time Augmentation (TTA)\n\nTo potentially improve our prediction accuracy, we will use **Test-Time Augmentation (TTA)**. This technique involves creating multiple augmented versions of each test image, making a prediction for each version, and then averaging these predictions. This process can lead to more robust and accurate results as it reduces the impact of random variations in the test images.","metadata":{}},{"cell_type":"code","source":"data_augmentation = tf.keras.Sequential([\n    # Geometric Transformations\n    tf.keras.layers.RandomRotation(40/ 360), # Randomly rotate images\n    tf.keras.layers.RandomTranslation(0.2, 0.2), # Randomly shift images horizontally and vertically\n    tf.keras.layers.RandomZoom(0.2, 0.2), # Randomly zoom into images\n    tf.keras.layers.RandomFlip('horizontal'), # Randomly flip images horizontally\n    tf.keras.layers.RandomFlip('vertical') # Randomly flip images vertically\n], name=\"data_augmentation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T18:19:08.472289Z","iopub.execute_input":"2025-08-19T18:19:08.472832Z","iopub.status.idle":"2025-08-19T18:19:08.491548Z","shell.execute_reply.started":"2025-08-19T18:19:08.472803Z","shell.execute_reply":"2025-08-19T18:19:08.490958Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Create the test dataset pipeline\nTEST_DIR = os.path.join(DATA_DIR, 'test_tfrecords')\ntest_files = tf.io.gfile.glob(os.path.join(TEST_DIR, '*.tfrec'))\ntest_dataset = tf.data.TFRecordDataset(test_files, num_parallel_reads= AUTO)\ntest_dataset = (test_dataset\n                .map(decode_test_example, num_parallel_calls= AUTO)\n                .map(lambda image, image_id: (preprocess_input(image), image_id), num_parallel_calls= AUTO)\n                .batch(BATCH_SIZE)\n                .prefetch(AUTO))\n\nprint('Test dataset created Successfully !')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T18:19:39.754352Z","iopub.execute_input":"2025-08-19T18:19:39.754993Z","iopub.status.idle":"2025-08-19T18:19:39.805355Z","shell.execute_reply.started":"2025-08-19T18:19:39.754953Z","shell.execute_reply":"2025-08-19T18:19:39.804596Z"}},"outputs":[{"name":"stdout","text":"Test dataset created Successfully !\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Function to get TTA predictions for a full batch of images\ndef tta_predict_batch(model, images, ids, num_tta):\n    \"\"\"\n    Run Test-Time Augmentation (TTA) on a batch of images.\n\n    Args:\n        model: Trained keras model\n        images: Batch of test images\n        ids: Batch of image ids (filenames)\n        num_tta: Number of augmentations per image\n\n    Returns:\n        preds: Averaged predictions for this batch\n        names: List of image ids corresponding to preds\n    \"\"\"\n    all_preds = []\n\n    for _ in range(num_tta):\n        # Apply augmentation to the *whole batch*\n        augmented = data_augmentation(images, training=True)\n\n        # Preprocess for EfficientNet\n        preprocessed = preprocess_input(augmented)\n\n        # Run inference on GPU (much faster in batch mode)\n        preds = model.predict(preprocessed, verbose=1)\n\n        all_preds.append(preds)\n\n    # Average predictions across TTA rounds\n    mean_preds = np.mean(all_preds, axis=0)\n\n    # Convert ids from tf.Tensors to python strings\n    names = [img_id.numpy().decode(\"utf-8\") for img_id in ids]\n\n    return mean_preds, names\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T18:19:44.288611Z","iopub.execute_input":"2025-08-19T18:19:44.289102Z","iopub.status.idle":"2025-08-19T18:19:44.294198Z","shell.execute_reply.started":"2025-08-19T18:19:44.289077Z","shell.execute_reply":"2025-08-19T18:19:44.293529Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Iterate through the test dataset and generate TTA predictions for each image\ntta_num_augmentations = 10  # Number of augmented images to create per test image\ntta_predictions = []\ntta_image_names = []\n\nfor images, ids in test_dataset:\n    # Run TTA for this *batch* (instead of each image individually)\n    mean_preds, names = tta_predict_batch(model, images, ids, tta_num_augmentations)\n\n    # Save results\n    tta_predictions.extend(mean_preds)\n    tta_image_names.extend(names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T18:19:44.560091Z","iopub.execute_input":"2025-08-19T18:19:44.560808Z","iopub.status.idle":"2025-08-19T18:19:53.640502Z","shell.execute_reply.started":"2025-08-19T18:19:44.560783Z","shell.execute_reply":"2025-08-19T18:19:53.639907Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1755627590.893080      97 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## üìù Create Submission File\n\nFinally, we process our predictions and format them into a `submission.csv` file as required by the competition. We take the `argmax` of our averaged predictions to get the final predicted class label for each image.","metadata":{}},{"cell_type":"code","source":"tta_predictions = np.array(tta_predictions)\n# Find the index of the highest probability for each prediction to get the final label\nfinal_tta_labels = tf.argmax(tta_predictions, axis=1)\npred_labels = final_tta_labels.numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T18:20:03.453827Z","iopub.execute_input":"2025-08-19T18:20:03.454566Z","iopub.status.idle":"2025-08-19T18:20:03.473306Z","shell.execute_reply.started":"2025-08-19T18:20:03.454541Z","shell.execute_reply":"2025-08-19T18:20:03.472592Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Create a new DataFrame with the correct order\nsubmission_df = pd.DataFrame({\n    'image_id': tta_image_names,\n    'label': pred_labels\n})\n\n# Save the final submission file\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint('Submission file created successfully!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T18:20:05.474997Z","iopub.execute_input":"2025-08-19T18:20:05.475285Z","iopub.status.idle":"2025-08-19T18:20:05.486807Z","shell.execute_reply.started":"2025-08-19T18:20:05.475265Z","shell.execute_reply":"2025-08-19T18:20:05.485968Z"}},"outputs":[{"name":"stdout","text":"Submission file created successfully!\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}