{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T16:41:29.148077Z",
     "iopub.status.busy": "2025-07-22T16:41:29.147596Z",
     "iopub.status.idle": "2025-07-22T16:41:29.156941Z",
     "shell.execute_reply": "2025-07-22T16:41:29.155511Z",
     "shell.execute_reply.started": "2025-07-22T16:41:29.148048Z"
    }
   },
   "source": [
    "# **Titanic: Predicting Survival with Advanced Feature Engineering & a Neural Network**\n",
    "\n",
    "Welcome to this analysis of the Titanic dataset! The sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.\n",
    "\n",
    "### **Goal**\n",
    "The primary goal of this notebook is to build a predictive model that can accurately determine whether a passenger would have survived the disaster. We will use passenger data (e.g., age, sex, class, fare) to make this prediction.\n",
    "\n",
    "### **Workflow**\n",
    "This notebook will follow a structured machine learning workflow:\n",
    "\n",
    "1.  **Data Loading & Initial Inspection:** We'll start by loading the training and testing datasets and performing a preliminary inspection to understand their structure, data types, and identify missing values.\n",
    "2.  **Exploratory Data Analysis (EDA):** We will dive deep into the data, using visualizations to uncover patterns, relationships, and insights about how different features correlate with survival.\n",
    "3.  **Feature Engineering & Preprocessing:** This is the core of our approach. We will:\n",
    "    * Impute missing values, most notably using an ensemble of machine learning models to predict `Age`.\n",
    "    * Engineer a rich set of new features from existing ones to capture complex relationships (e.g., family size, social status, ticket characteristics).\n",
    "    * Encode categorical variables and scale numerical features to prepare the data for our model.\n",
    "4.  **Modeling:** We will build, train, and evaluate a Neural Network using TensorFlow and Keras. The model is designed to learn from the complex features we've created.\n",
    "5.  **Prediction & Submission:** Finally, we'll use our trained model to predict survival for the passengers in the test set and generate a submission file for the Kaggle competition.\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Importing Libraries**\n",
    "First, we'll import all the necessary Python libraries for data manipulation, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Manipulation ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "\n",
    "# --- Data Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Machine Learning & Preprocessing ---\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# --- Utilities ---\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Ignore warning messages for cleaner output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "SEED = 28\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Data Loading**\n",
    "Here, we load the training and test datasets from the provided CSV files. We'll create copies to ensure the original dataframes remain untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets from CSV files\n",
    "df_train_raw = pd.read_csv('../input/titanic/train.csv')\n",
    "df_test_raw = pd.read_csv('../input/titanic/test.csv')\n",
    "\n",
    "# Create copies for manipulation\n",
    "dftrain = df_train_raw.copy()\n",
    "dftest = df_test_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1. Initial Data Inspection**\n",
    "Let's take a first look at the training data to understand its columns and the type of data they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the training dataframe\n",
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now a look at the test data. Note that it lacks the `Survived` column, as this is what we need to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the test dataframe\n",
    "dftest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.info()` method provides a concise summary of the dataframe, including the data type of each column and the number of non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of the training data, including data types and non-null counts\n",
    "dftrain.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same for the test set to check for consistency and identify missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of the test data\n",
    "dftest.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quantify the missing values in the training set. `Age`, `Cabin`, and `Embarked` have missing entries that we'll need to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of null (missing) values in each column of the training set\n",
    "dftrain.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we check for missing values in the test set. `Age`, `Fare`, and `Cabin` are the columns with missing data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of null (missing) values in each column of the test set\n",
    "dftest.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.describe()` method gives us statistical details like mean, standard deviation, and quartiles for the numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics for the numerical columns in the training set\n",
    "dftrain.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Exploratory Data Analysis (EDA)**\n",
    "Now, let's visualize the data to uncover insights and relationships between features and the survival outcome.\n",
    "\n",
    "### **3.1. Distribution of Survival**\n",
    "First, we'll see the distribution of our target variable, `Survived`. (0 = Did not survive, 1 = Survived)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the 'Survived' column\n",
    "sns.countplot(x='Survived', data=dftrain)\n",
    "plt.title('Distribution of Survival (0 = No, 1 = Yes)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2. Age Distribution**\n",
    "Next, let's look at the age distribution of the passengers. This will be helpful before we decide on a strategy for imputing missing ages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of passenger ages\n",
    "sns.histplot(dftrain.Age.dropna(), kde=True)\n",
    "plt.title('Age Distribution of Passengers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3. Counts of Categorical Features**\n",
    "Here, we'll visualize the counts for key categorical features to understand their composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categorical columns to plot\n",
    "countplot_cols = ['Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked']\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 10))\n",
    "axs = axs.flatten() # Flatten the axes array for easy iteration\n",
    "\n",
    "# Loop through the columns and create a count plot for each\n",
    "for i, col in enumerate(countplot_cols):\n",
    "    sns.countplot(x=col, data=dftrain, ax=axs[i])\n",
    "    axs[i].set_title(f'{col} Counts')\n",
    "    axs[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.4. Survival Analysis by Feature**\n",
    "\n",
    "Now for the most important part of EDA: how do different features relate to survival?\n",
    "\n",
    "We will create new features (`title`, `FareGroup`, `has_cabin`) and then plot both the survival count and survival rate for each category within our key features. This will give us a clear idea of which groups had a higher chance of survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Engineering for EDA ---\n",
    "\n",
    "# Extract titles (e.g., Mr, Mrs, Miss) from the 'Name' column\n",
    "dftrain['title'] = dftrain['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "# Consolidate titles into broader categories\n",
    "title_mapping = {\n",
    "    'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',\n",
    "    'Dr': 'Officer', 'Rev': 'Officer', 'Col': 'Officer', 'Major': 'Officer',\n",
    "    'Mlle': 'Miss', 'Countess': 'Royal', 'Ms': 'Mrs', 'Lady': 'Royal',\n",
    "    'Jonkheer': 'Royal', 'Don': 'Royal', 'Dona': 'Royal', 'Mme': 'Mrs',\n",
    "    'Capt': 'Officer', 'Sir': 'Royal'\n",
    "}\n",
    "dftrain['title'] = dftrain['title'].map(title_mapping).fillna('Rare')\n",
    "\n",
    "# Create fare groups by splitting 'Fare' into 4 quantiles (quartiles)\n",
    "dftrain['FareGroup'] = pd.qcut(dftrain['Fare'], 4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "# Create a binary feature indicating if a passenger had a cabin number recorded\n",
    "dftrain['has_cabin'] = dftrain['Cabin'].notnull().astype(int)\n",
    "\n",
    "# --- Visualization ---\n",
    "\n",
    "# List of features to analyze against survival\n",
    "survival_rate_cols = ['Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked', 'title', 'FareGroup', 'has_cabin']\n",
    "\n",
    "# Setup for subplots\n",
    "n_cols = 2\n",
    "n_rows = len(survival_rate_cols)\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(12, 4 * n_rows))\n",
    "fig.suptitle('Survival Analysis by Feature', fontsize=20, y=1.02)\n",
    "\n",
    "# Generate plots for each feature\n",
    "for i, col in enumerate(survival_rate_cols):\n",
    "    # Plot 1: Survival Count (e.g., how many men survived vs. didn't)\n",
    "    sns.countplot(x=col, hue='Survived', data=dftrain, ax=axs[i, 0])\n",
    "    axs[i, 0].set_title(f'Survived vs. {col}')\n",
    "    axs[i, 0].tick_params(axis='x', rotation=45)\n",
    "    axs[i, 0].legend(title='Survived', labels=['No', 'Yes'])\n",
    "\n",
    "    # Plot 2: Survival Rate (e.g., what percentage of men survived)\n",
    "    sns.barplot(x=col, y='Survived', data=dftrain, ax=axs[i, 1])\n",
    "    axs[i, 1].set_title(f'Survival Rate by {col}')\n",
    "    axs[i, 1].set_ylabel('Survival Rate')\n",
    "    axs[i, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 1]) # Adjust layout to make space for suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.5. Survival by Fare**\n",
    "A box plot is a great way to see the distribution of `Fare` for those who survived versus those who didn't. It appears that passengers who paid a higher fare had a better chance of survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a box plot to show the distribution of 'Fare' for each survival outcome\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='Survived', y='Fare', data=dftrain)\n",
    "plt.title('Fare Distribution by Survival')\n",
    "plt.ylabel(\"Fare\")\n",
    "plt.xlabel(\"Survived (0 = No, 1 = Yes)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.6. Survival by Age Group**\n",
    "To better analyze the relationship between age and survival, we can group ages into bins and calculate the survival rate for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create age bins to group passengers\n",
    "dftrain['AgeGroup'] = pd.cut(dftrain['Age'], bins=[0, 10, 20, 30, 40, 50, 60, 80],\n",
    "                             labels=['0-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61+'])\n",
    "\n",
    "# Plot the survival rate for each age group\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='AgeGroup', y='Survived', data=dftrain)\n",
    "plt.title('Survival Rate by Age Group')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.xlabel('Age Group')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.7. Age Distribution by Survival (KDE)**\n",
    "A Kernel Density Estimate (KDE) plot provides a smoothed view of the age distribution for survivors and non-survivors. We can see that young children had a higher survival rate, while young adults (20-40) had a lower rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot KDE for survived passengers\n",
    "sns.kdeplot(dftrain[dftrain['Survived'] == 1]['Age'], label='Survived', shade=True, color='green')\n",
    "# Plot KDE for non-survived passengers\n",
    "sns.kdeplot(dftrain[dftrain['Survived'] == 0]['Age'], label='Not Survived', shade=True, color='red')\n",
    "\n",
    "plt.title('Age Distribution by Survival')\n",
    "plt.xlabel('Age')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Feature Engineering & Preprocessing**\n",
    "\n",
    "This is the most critical stage. We will create three main functions:\n",
    "1. `feature_engineering_for_age`: A sophisticated function based on some features that contains some features created based on correlation\n",
    "2.  `impute_age`: A sophisticated function to fill missing `Age` values using an ensemble of LightGBM and Random Forest models.\n",
    "3.  `feature_engineering`: A comprehensive function to create all our features, handle missing values, and encode categorical data.\n",
    "\n",
    "This structured approach ensures that both the training and test data are processed identically.  \n",
    "First lets see correlation for `Age`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1. Feature Engineering for age Function**  \n",
    "First lets see correlation related `Age`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_train_raw.copy()\n",
    "cat_feat = ['Sex', 'Embarked']\n",
    "for col in cat_feat:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "features_for_corr = ['Age', 'Parch', 'SibSp', 'Fare', \n",
    "                     'Pclass', 'Sex', 'Embarked']\n",
    "corr = df_encoded[features_for_corr].corr()\n",
    "plt.figure(figsize= (8, 6))\n",
    "sns.heatmap(corr, annot= True)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see these features doesn't related too much too Age.Let's see what we can do to created some better features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_for_age(df):\n",
    "    \"\"\"\n",
    "    Performs feature engineering to create new variables and handle initial missing values.\n",
    "    This function prepares features useful for general modeling tasks and especially for \n",
    "    predicting missing 'Age' values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe (train or test).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, List[str]]: The dataframe with engineered features,\n",
    "        and a list of feature names useful for imputing 'Age'.\n",
    "    \"\"\"\n",
    "\n",
    "    # ========== Handling Missing Values ==========\n",
    "    \n",
    "    # Impute missing 'Fare' with the median fare of each passenger class\n",
    "    if df['Fare'].isnull().any():\n",
    "        median_fare = df.groupby('Pclass')['Fare'].transform('median')\n",
    "        df['Fare'].fillna(median_fare, inplace=True)\n",
    "    \n",
    "    # Fill missing 'Embarked' with the most frequent port\n",
    "    df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "    \n",
    "    # ========== Feature Extraction ==========\n",
    "\n",
    "    # Extract title from passenger names (e.g., Mr, Miss, Dr)\n",
    "    df['title'] = df['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "    # Binary indicator for presence of cabin information\n",
    "    df['has_cabin'] = df['Cabin'].notnull().astype(int)\n",
    "\n",
    "    # Calculate family size including the passenger\n",
    "    df['family_size'] = df['SibSp'] + df['Parch'] + 1\n",
    "\n",
    "    # Extract deck letter from Cabin (can be multiple for shared cabins)\n",
    "    df['decks'] = df['Cabin'].str.findall(r'([A-Za-z])')\n",
    "\n",
    "    # Count how many people share the same ticket (ticket group size)\n",
    "    df['ticket_group_size'] = df.groupby('Ticket')['Ticket'].transform('count')\n",
    "\n",
    "    # ========== Title Grouping and Encoding ==========\n",
    "\n",
    "    # Map raw titles to broader, standardized categories\n",
    "    title_mapping = {\n",
    "        'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',\n",
    "        'Dr': 'Officer', 'Rev': 'Officer', 'Col': 'Officer', 'Major': 'Officer',\n",
    "        'Mlle': 'Miss', 'Countess': 'Royal', 'Ms': 'Mrs', 'Lady': 'Royal',\n",
    "        'Jonkheer': 'Royal', 'Don': 'Royal', 'Dona': 'Royal', 'Mme': 'Mrs',\n",
    "        'Capt': 'Officer', 'Sir': 'Royal'\n",
    "    }\n",
    "    df['title'] = df['title'].map(title_mapping).fillna('Rare')\n",
    "\n",
    "    # Fill missing decks with placeholder\n",
    "    df['decks'] = df['decks'].fillna('Unknown')\n",
    "\n",
    "    # Encode title as numerical feature\n",
    "    le = LabelEncoder()\n",
    "    df['title_encoded'] = le.fit_transform(df['title'])\n",
    "\n",
    "    # ========== Derived and Interaction Features ==========\n",
    "\n",
    "    # Indicator: whether the passenger is a child (Master)\n",
    "    df['is_child'] = (df['title'] == 'Master').astype(int)\n",
    "\n",
    "    # Indicator: whether the passenger is likely a parent (Mr or Mrs with family)\n",
    "    df['is_parent'] = ((df['title'].isin(['Mr', 'Mrs'])) & (df['family_size'] > 1)).astype(int)\n",
    "\n",
    "    # Cabin wealth feature: fare adjusted by cabin presence\n",
    "    df['cabin_wealth'] = df['has_cabin'] * np.log1p(df['Fare'])\n",
    "\n",
    "    # Difference between total family size and number of parents/children\n",
    "    df['family_size_parch'] = df['family_size'] - df['Parch']\n",
    "\n",
    "    # Interaction of ticket group size with passenger class\n",
    "    df['ticket_size_class'] = df['ticket_group_size'] * df['Pclass']\n",
    "\n",
    "    # Interaction categorical features for modeling\n",
    "    df['title_decks'] = df['title'].astype(str) + '_' + df['decks'].astype(str)\n",
    "    df['title_sex'] = df['title'].astype(str) + '_' + df['Sex'].astype(str)\n",
    "\n",
    "    # Numerical interaction features\n",
    "    df['title_family_size'] = df['title_encoded'] * df['family_size']\n",
    "    df['title_fare'] = df['title_encoded'] * df['Fare']\n",
    "\n",
    "    # ========== Features Useful for Age Imputation ==========\n",
    "    age_features = [\n",
    "        'title_encoded', 'title_decks', 'title_sex', 'is_parent', 'cabin_wealth',\n",
    "        'title_fare', 'title_family_size', 'ticket_group_size', 'is_child',\n",
    "        'family_size_parch', 'ticket_size_class'\n",
    "    ]\n",
    "\n",
    "    # ========== Metadata and Logging ==========\n",
    "    df.attrs['name'] = 'Raw DataFrame'\n",
    "    print(f\"The {df.attrs['name']} has filled these new features for missing age imputation.\")\n",
    "    print(f\"New features used for imputing age:\\n{', '.join(age_features)}\")\n",
    "\n",
    "    return df, age_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use our function to create a new DataFrame.  \n",
    "**Note** : We need to keep these new `DataFrames` to use it later. for more robusted way "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_raw2, age_features = feature_engineering_for_age(df_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_raw2, _ = feature_engineering_for_age(df_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see new columns in train set.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_raw2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see now what `correlation` for *age* receive with this new features that we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc = df_train_raw2.copy()\n",
    "df_enc.drop(['Survived', 'Ticket', 'Name', 'PassengerId', 'Cabin'], axis= 1, inplace= True)\n",
    "cat_feat = ['title_decks', 'title_sex', 'Sex']\n",
    "for col in cat_feat:\n",
    "    le = LabelEncoder()\n",
    "    df_enc[col] = le.fit_transform(df_enc[col].astype(str))\n",
    "\n",
    "corr_feat = ['Age', 'ticket_group_size', 'title_encoded', 'is_child',\n",
    "       'is_parent', 'cabin_wealth', 'family_size_parch', 'ticket_size_class',\n",
    "       'title_decks', 'title_sex', 'title_family_size', 'title_fare', \n",
    "]\n",
    "corr = df_enc[corr_feat].corr()\n",
    "plt.figure(figsize= (10, 10))\n",
    "sns.heatmap(corr, annot= True)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you see now we have stronger `features` to impute age based on these.  \n",
    "Let's do it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Age Imputation with LightGBM and Random Forest\n",
    "\n",
    "This function fills missing `Age` values using a weighted average of predictions from:\n",
    "\n",
    "- **LightGBM** (default weight = 0.6)\n",
    "- **Random Forest** (default weight = 0.4)\n",
    "\n",
    "It trains both models on rows with known ages using `age_features`, then imputes the missing values. Predictions are clipped between 0 and 80 for realism.\n",
    "\n",
    "**Returns:** Updated DataFrame with imputed `Age`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_age(df, age_features= age_features, lgb_weight=0.6, rf_weight=0.4):\n",
    "    \"\"\"\n",
    "    Imputes missing 'Age' values using a weighted ensemble of LightGBM and\n",
    "    Random Forest regressors.\n",
    "\n",
    "    This function:\n",
    "    - Splits the data into rows with known and missing ages.\n",
    "    - Trains LightGBM and Random Forest models on known age data.\n",
    "    - Predicts the missing ages using both models.\n",
    "    - Creates a weighted ensemble of the predictions.\n",
    "    - Fills in the missing 'Age' values using the ensemble output.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe (train or test).\n",
    "        age_features (List[str]): The feature list used to predict age.\n",
    "        lgb_weight (float): Weight of LightGBM model in the ensemble.\n",
    "        rf_weight (float): Weight of Random Forest model in the ensemble.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with missing 'Age' values imputed.\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original dataframe\n",
    "    df_temp = df.copy()\n",
    "\n",
    "    # --- Step 2: Split data into known and unknown age groups ---\n",
    "    age_known_df = df_temp[df_temp['Age'].notnull()]\n",
    "    age_missing_df = df_temp[df_temp['Age'].isnull()]\n",
    "\n",
    "    # If no ages are missing, return the original dataframe\n",
    "    if age_missing_df.empty:\n",
    "        return df\n",
    "\n",
    "    # Prepare training data (X) and target (y)\n",
    "    X_train_age = age_known_df[age_features]\n",
    "    y_train_age = age_known_df['Age']\n",
    "    X_pred_age = age_missing_df[age_features]\n",
    "    \n",
    "    # Prepare data for both models\n",
    "    X_train_lgb = X_train_age.copy()\n",
    "    X_pred_lgb = X_pred_age.copy()\n",
    "    X_train_rf = X_train_age.copy()\n",
    "    X_pred_rf = X_pred_age.copy()\n",
    "\n",
    "    # --- Step 3: Train and predict with LightGBM ---\n",
    "    categorical_cols = ['title_sex', 'title_decks', 'title_encoded']\n",
    "    for col in categorical_cols:\n",
    "        X_train_lgb[col] = X_train_lgb[col].astype('category')\n",
    "        X_pred_lgb[col] = X_pred_lgb[col].astype('category')\n",
    "    lgb_model = LGBMRegressor(\n",
    "        objective='regression',\n",
    "        n_estimators=80,\n",
    "        learning_rate=0.08,\n",
    "        num_leaves=15,\n",
    "        min_child_samples=15,\n",
    "        min_split_gain=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1,\n",
    "        random_state=SEED,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    lgb_model.fit(X_train_lgb, y_train_age, categorical_feature=categorical_cols)\n",
    "    lgb_pred = lgb_model.predict(X_pred_lgb)\n",
    "\n",
    "    # --- Step 4: Train and predict with Random Forest ---\n",
    "    # Encode categorical features for Random Forest\n",
    "    for col in ['title_decks', 'title_sex']:\n",
    "        le = LabelEncoder()\n",
    "        X_train_rf[col] = le.fit_transform(X_train_rf[col])\n",
    "        X_pred_rf[col] = le.fit_transform(X_pred_rf[col])\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=150,\n",
    "        max_depth=12,\n",
    "        min_samples_split=8,\n",
    "        min_samples_leaf=4,\n",
    "        max_features='sqrt',\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_model.fit(X_train_rf, y_train_age)\n",
    "    rf_pred = rf_model.predict(X_pred_rf)\n",
    "\n",
    "    # --- Step 5: Create weighted ensemble prediction ---\n",
    "    # Combine predictions, giving slightly more weight to LGBM\n",
    "    ensemble_pred = (lgb_weight * lgb_pred) + (rf_weight * rf_pred)\n",
    "    ensemble_pred = np.clip(np.round(ensemble_pred), 0, 80) # Clip predictions to a realistic age range\n",
    "\n",
    "    \n",
    "    # --- Step 6: Fill in the missing ages in the original dataframe ---\n",
    "    missing_age_indices = df[df['Age'].isnull()].index\n",
    "\n",
    "    if len(missing_age_indices) == len(ensemble_pred):\n",
    "        df.loc[missing_age_indices, 'Age'] = ensemble_pred\n",
    "        print(f\"Filled {len(ensemble_pred)} missing 'Age' values.\")\n",
    "        print(f\"LightGBM weight: {lgb_weight}, Random Forest weight: {rf_weight}\")\n",
    "    else:\n",
    "        print(\"⚠️ Warning: Length mismatch in age imputation. Skipping filling step.\")\n",
    "        print(f\"Expected {len(missing_age_indices)} values, but got {len(ensemble_pred)} predictions.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Temporary Engineered Features\n",
    "\n",
    "This function removes temporary features (e.g., used for age imputation) from the DataFrame:\n",
    "\n",
    "- `title_encoded`, `decks`, `title_family_size`, `title_fare`, `title_decks`, `title_sex`\n",
    "\n",
    "It returns a cleaned DataFrame with those columns dropped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_cleaning(df):\n",
    "    \"\"\"\n",
    "    Removes temporary or intermediate features from the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame to clean.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame without the specified features.\n",
    "    \"\"\"\n",
    "    # List of engineered features used earlier (e.g., for age imputation) that are no longer needed\n",
    "    cleaned_features = ['title_encoded', 'decks', 'title_family_size', \n",
    "                        'title_fare', 'title_decks', 'title_sex']\n",
    "    \n",
    "    # Drop these columns from the DataFrame\n",
    "    df.drop(cleaned_features, axis=1, inplace=True)\n",
    "\n",
    "    # Log the cleanup\n",
    "    print(f\"{', '.join(cleaned_features)} has cleaned from DataFrame\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to Imputation for $Age$ , we use last dataframe `df_train_raw2` and `age_features` for train dataset and `df_test_raw2` for test dataset.  \n",
    "Then clean redundant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_age_filled = impute_age(df_train_raw2, age_features, lgb_weight=0.6, rf_weight= 0.4)\n",
    "df_test_age_filled = impute_age(df_test_raw2, age_features, lgb_weight=0.6, rf_weight= 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_intermediate = df_cleaning(df_train_age_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_intermediate = df_cleaning(df_test_age_filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : As said before, keep the intermediate DataFrames for better inference and avoiding from creating some `features` again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.3. Main Feature Engineering Function**\n",
    "This function is the heart of our data preparation pipeline. It takes an `intermediate` dataframe that created after age imputation and transforms it into a feature-rich dataset ready for a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting All Encoders on Train and Test Data\n",
    "\n",
    "The function below fits all LabelEncoders required for feature engineering on the union of both train and test datasets.  \n",
    "This ensures every possible class (including 'Unknown' or 'None') is recognized and prevents unseen-label errors when transforming either dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_all_encoders(df_train_raw, df_test_raw):\n",
    "    \"\"\"\n",
    "    Fits label encoders on combined train and test sets for various categorical features\n",
    "    to ensure consistent encoding across both datasets.\n",
    "\n",
    "    Args:\n",
    "        df_train_raw (pd.DataFrame): Raw training DataFrame.\n",
    "        df_test_raw (pd.DataFrame): Raw test DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing fitted LabelEncoders for:\n",
    "            - title_grouped\n",
    "            - primary_deck\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Title Grouped ---\n",
    "    # Extract and normalize titles from names, then map them to grouped categories\n",
    "    train_titles = df_train_raw['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    test_titles = df_test_raw['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    \n",
    "    title_mapping = {\n",
    "        'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master', 'Dr': 'Officer',\n",
    "        'Rev': 'Officer', 'Col': 'Officer', 'Major': 'Officer', 'Mlle': 'Miss',\n",
    "        'Countess': 'Royal', 'Ms': 'Mrs', 'Lady': 'Royal', 'Jonkheer': 'Royal',\n",
    "        'Don': 'Royal', 'Dona': 'Royal', 'Mme': 'Mrs', 'Capt': 'Officer', 'Sir': 'Royal'\n",
    "    }\n",
    "    \n",
    "    all_titles = pd.concat([train_titles, test_titles], ignore_index=True)\n",
    "    all_titles_grouped = all_titles.map(title_mapping).fillna('Rare').unique()\n",
    "    \n",
    "    le_title_grouped = LabelEncoder()\n",
    "    le_title_grouped.fit(all_titles_grouped)\n",
    "\n",
    "    # --- Primary Deck ---\n",
    "    # Extract first letter from Cabin as deck information, fill missing with 'Unknown'\n",
    "    train_decks = df_train_raw['Cabin'].str.extract(r'^([A-Za-z])', expand=False).fillna('Unknown')\n",
    "    test_decks = df_test_raw['Cabin'].str.extract(r'^([A-Za-z])', expand=False).fillna('Unknown')\n",
    "    \n",
    "    all_decks = pd.concat([train_decks, test_decks], ignore_index=True).unique()\n",
    "    \n",
    "    le_deck = LabelEncoder()\n",
    "    le_deck.fit(all_decks)\n",
    "\n",
    "    return {\n",
    "        'title_grouped': le_title_grouped,\n",
    "        'primary_deck': le_deck\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Feature Engineering for Neural Network Models\n",
    "\n",
    "This function builds on top of the earlier `feature_engineering_for_age()`.\n",
    "\n",
    "✅ Assumptions:\n",
    "- Some features like `title`, `family_size`, `has_cabin`, and `cabin_wealth` already exist.\n",
    "- Temporary features created earlier (e.g. `title_decks`, `title_sex`) have already been cleaned.\n",
    "\n",
    "### What it adds:\n",
    "- Encoded categorical features: `title_encoded`, `deck_encoded`, `embarked_encoded`, etc.\n",
    "- Interaction features tailored for neural networks.\n",
    "- New numerical features like `fare_per_person_log`, `social_status`, and more.\n",
    "- Logical groups for wealth, family, survival relevance, and social status.\n",
    "\n",
    "Returns the final, enhanced DataFrame for deep learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df, encoders):\n",
    "    \"\"\"\n",
    "    Enhanced feature engineering specifically optimized for neural networks.\n",
    "    Assumes some features were already created in feature_engineering_for_age and later cleaned.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Deck and cabin counts\n",
    "    df['num_decks'] = df['Cabin'].apply(\n",
    "        lambda x: 1 if pd.isnull(x) else len(re.findall(r'[A-Za-z]', x))\n",
    "    )\n",
    "    df['num_cabins'] = df['Cabin'].apply(lambda x: 1 if pd.isnull(x) else len(x.split()))\n",
    "    \n",
    "    # Alone indicator\n",
    "    df['is_alone'] = (df['family_size'] == 1).astype(int)\n",
    "    \n",
    "    # Primary deck extraction and cleanup\n",
    "    df['primary_deck'] = df['Cabin'].str.extract(r'^([A-Za-z])', expand=False)\n",
    "    df['primary_deck'] = df['primary_deck'].fillna('Unknown')\n",
    "    df['Cabin'] = df['Cabin'].fillna('Unknown')\n",
    "    \n",
    "    # Family type classification\n",
    "    df['family_type'] = 'Single'\n",
    "    df.loc[df['family_size'] == 2, 'family_type'] = 'Couple'\n",
    "    df.loc[(df['family_size'] >= 3) & (df['family_size'] <= 4), 'family_type'] = 'Small_Family'\n",
    "    df.loc[df['family_size'] >= 5, 'family_type'] = 'Large_Family'\n",
    "    \n",
    "    # Ticket prefix and binary indicator\n",
    "    df['ticket_prefix'] = df['Ticket'].str.extract('([A-Za-z]+)', expand=False).fillna('None')\n",
    "    df['has_ticket_prefix'] = (df['ticket_prefix'] != 'None').astype(int)\n",
    "    \n",
    "    # Encoding Sex\n",
    "    df['sex_encoded'] = df['Sex'].map({'male': 1, 'female': 0})\n",
    "\n",
    "    # Label encode using pre-fitted encoders\n",
    "    df['title_encoded'] = encoders['title_grouped'].transform(df['title'])\n",
    "    df['deck_encoded'] = encoders['primary_deck'].transform(df['primary_deck'])\n",
    "\n",
    "    # Log-transform fares\n",
    "    df['log_fare'] = np.log1p(df['Fare'])\n",
    "    df['fare_per_person'] = df['Fare'] / df['family_size']\n",
    "    df['fare_per_person_log'] = np.log1p(df['fare_per_person'])\n",
    "\n",
    "    # Bin fare and age into quantiles and groups\n",
    "    df['fare_quartile'] = pd.qcut(df['Fare'], 4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "    df['age_group'] = pd.cut(df['Age'], bins=[0, 12, 18, 35, 60, 100], labels=['Child', 'Teenager', 'Adult', 'Old', 'Very Old'])\n",
    "\n",
    "    # Interaction features for NN\n",
    "    df['age_class'] = df['Age'] * df['Pclass']\n",
    "    df['fare_class'] = df['log_fare'] * df['Pclass']\n",
    "    df['sex_class'] = df['sex_encoded'] * df['Pclass']\n",
    "    df['title_class'] = df['title_encoded'] * df['Pclass']\n",
    "    df['deck_class'] = df['deck_encoded'] * df['Pclass']\n",
    "    df['family_fare'] = df['family_size'] * df['log_fare']\n",
    "    df['title_age'] = df['title_encoded'] * df['Age']\n",
    "    df['cabin_class'] = df['has_cabin'] * df['Pclass']\n",
    "\n",
    "    # Social status interaction\n",
    "    df['title_family_interaction'] = df['title_encoded'] * df['family_size']\n",
    "    df['wealth_indicator'] = df['cabin_wealth'] + df['log_fare'] + (df['Pclass'] * -1)\n",
    "\n",
    "    # Survival-related features\n",
    "    df['women_children_first'] = ((df['sex_encoded'] == 0) | (df['Age'] < 16)).astype(int)\n",
    "    df['adult_male'] = ((df['sex_encoded'] == 1) & (df['Age'] >= 16)).astype(int)\n",
    "    df['family_with_cabin'] = df['family_size'] * df['has_cabin']\n",
    "\n",
    "    # Social status score based on multiple traits\n",
    "    df['social_status'] = (\n",
    "        (df['Pclass'] == 1).astype(int) * 3 +\n",
    "        df['has_cabin'] * 2 +\n",
    "        df['has_ticket_prefix'] * 1 +\n",
    "        (df['title_encoded'] == encoders['title_grouped'].transform(['Royal'])[0]) * 4\n",
    "    )\n",
    "\n",
    "    print(f\"All new features added Successfully!\\nAll Features:\\n{', '.join(df.columns)}\\nNumber of features: {len(df.columns)}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.3. Applying Feature Engineering**\n",
    "Now we apply the `feature_engineering` function to our raw training and test dataframes to create our final, processed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fit all encoders on the union of train & test data\n",
    "# IMPORTANT: USE INTERMEDIATE DFs\n",
    "encoders = fit_all_encoders(df_train_raw, df_test_raw)\n",
    "\n",
    "# Process the training data\n",
    "df_train_processed = feature_engineering(df_train_intermediate, encoders= encoders)\n",
    "\n",
    "# Process the test data\n",
    "df_test_processed = feature_engineering(df_test_intermediate, encoders= encoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our new `DataFrames` again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_processed.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_processed.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Model Preparation**\n",
    "We need to select the final set of features that will be fed into our neural network and then scale them.\n",
    "\n",
    "### **5.1. Define Feature Set**\n",
    "Here, we curate a list of all the engineered and encoded features we want to use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_features():\n",
    "    \"\"\"\n",
    "    Returns separate lists of features grouped by type:\n",
    "    - categorical_features\n",
    "    - numerical_features\n",
    "    - binary_features\n",
    "    \"\"\"\n",
    "    categorical_features = [\n",
    "        'Pclass', 'Sex', 'fare_quartile', 'title', 'primary_deck',\n",
    "        'Embarked', 'ticket_prefix', 'age_group', 'family_type'\n",
    "    ]\n",
    "\n",
    "    numerical_features = [\n",
    "        'Age', 'SibSp', 'Parch', 'family_size', 'family_size_parch',\n",
    "        'family_with_cabin', 'fare_per_person_log', 'cabin_wealth',\n",
    "        'wealth_indicator', 'title_class', 'deck_class', 'social_status', 'num_cabins',\n",
    "        'num_decks', 'ticket_group_size', 'ticket_size_class', 'age_class',\n",
    "        'fare_class', 'sex_class', 'title_age', 'title_family_interaction',\n",
    "        'cabin_class', 'family_fare'\n",
    "    ]\n",
    "\n",
    "    binary_features = [\n",
    "        'is_alone', 'is_child', 'is_parent', 'has_cabin',\n",
    "        'has_ticket_prefix', 'women_children_first', 'adult_male'\n",
    "    ]\n",
    "\n",
    "    return categorical_features, numerical_features, binary_features\n",
    "\n",
    "categorical_features, numerical_features, binary_features = get_nn_features()\n",
    "\n",
    "print(f'Numerical Features count: {len(numerical_features)}')\n",
    "print(f'Categorical Features count: {len(categorical_features)}')\n",
    "print(f'Binary Features count: {len(binary_features)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2. Scaling and Encoding the Features**\n",
    "Neural networks perform best when numerical inputs are standardized and categorical variables are properly encoded.  \n",
    "We use `StandardScaler` to standardize numerical features (mean 0, variance 1) and `OneHotEncoder` to convert categorical features into binary indicator variables.\n",
    "\n",
    "#### Prepare Data for Neural Network Training\n",
    "\n",
    "This function prepares the dataset for neural network input by:\n",
    "\n",
    "- Splitting the data into **numerical**, **categorical**, and **binary** feature sets.\n",
    "- Extracting the target variable (`Survived`).\n",
    "- **Scaling numerical features** using `StandardScaler` for normalized input.\n",
    "- **One-hot encoding categorical features** to create meaningful binary columns.\n",
    "- Combining numerical, binary, and encoded categorical features into a single DataFrame for training and testing.\n",
    "\n",
    "Returns:\n",
    "- Final processed training DataFrame (`df_train_final`)\n",
    "- Target Series (`y_train`)\n",
    "- Final processed test DataFrame (`df_test_final`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_nn(df_train, df_test, numerical_features= None, categorical_features= None, binary_features= None):\n",
    "    \"\"\"\n",
    "    Final preparation for neural network training\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare training data\n",
    "    y_train = df_train['Survived']\n",
    "    X_train_num = df_train[numerical_features].copy()\n",
    "    X_test_num = df_test[numerical_features].copy()\n",
    "    X_train_cat = df_train[categorical_features].copy()\n",
    "    X_test_cat = df_test[categorical_features].copy()\n",
    "    df_train_bin = df_train[binary_features].copy()\n",
    "    df_test_bin = df_test[binary_features].copy()\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_num_scaled = scaler.fit_transform(X_train_num)\n",
    "    X_test_num_scaled = scaler.transform(X_test_num)\n",
    "    df_train_num = pd.DataFrame(X_train_num_scaled, columns= numerical_features, index= df_train.index)\n",
    "    df_test_num = pd.DataFrame(X_test_num_scaled, columns= numerical_features, index= df_test.index)\n",
    "    \n",
    "    # One Hot Encoding Categorical Features\n",
    "    ohe = OneHotEncoder(sparse_output= False, handle_unknown= 'ignore')\n",
    "    X_train_cat_ohe = ohe.fit_transform(X_train_cat)\n",
    "    X_test_cat_ohe = ohe.transform(X_test_cat)\n",
    "    \n",
    "    cat_columns = ohe.get_feature_names_out(categorical_features)\n",
    "    df_train_cat = pd.DataFrame(X_train_cat_ohe, columns= cat_columns, index= df_train.index)\n",
    "    df_test_cat = pd.DataFrame(X_test_cat_ohe, columns= cat_columns, index= df_test.index)\n",
    "    \n",
    "    # ---  Combine numerical and categorical features ---\n",
    "    df_train_final = pd.concat([df_train_num, df_train_bin, df_train_cat], axis= 1)\n",
    "    df_test_final = pd.concat([df_test_num, df_test_bin, df_test_cat], axis= 1)\n",
    "    \n",
    "    return df_train_final, y_train, df_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare Training Data ---\n",
    "# Select the feature columns\n",
    "df_train_final, y, df_test_final = prepare_for_nn(df_train_processed, df_test_processed, \n",
    "                                    numerical_features= numerical_features, categorical_features= categorical_features,\n",
    "                                    binary_features= binary_features)\n",
    "X_train = df_train_final.to_numpy()\n",
    "X_test = df_test_final.to_numpy()\n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Modeling**\n",
    "We now train our neural network using `TensorFlow` and `Keras`. To ensure robust evaluation, we apply `K-Fold` Cross-Validation. This method helps assess the model's generalization ability across different data splits.\n",
    "\n",
    "\n",
    "### **6.1. K-Fold Setup and Dataset Pipeline**\n",
    "We define constants and set up the training pipeline inside each fold. Each fold splits the data into training and validation sets. The input pipeline uses TensorFlow `tf.data.Dataset` for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for training\n",
    "NUM_FOLD = 9\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_SIZE = 1000\n",
    "PREFETCH_SIZE = tf.data.AUTOTUNE\n",
    "\n",
    "# Create folds\n",
    "kf = StratifiedKFold(NUM_FOLD, shuffle=True, random_state=SEED)\n",
    "\n",
    "# For logging\n",
    "fold_accuracies, fold_losses = [], []\n",
    "histories = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.2. Training Loop with K-Fold**\n",
    "For each fold, we:\n",
    "\n",
    "- Create `train` and `validation` datasets.\n",
    "\n",
    "- Define the `model` architecture.\n",
    "\n",
    "- Compile and train with callbacks.\n",
    "\n",
    "- Store fold `metrics` and `training history`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize the Training Dataset Pipeline\n",
    "\n",
    "We apply a series of performance optimizations to the `train_ds` and `val_ds` dataset:\n",
    "- `cache()` stores the dataset in memory after the first epoch (for small-to-medium datasets).\n",
    "- `shuffle()` randomizes the order of elements, helping prevent model overfitting.\n",
    "- `batch()` splits the data into batches for efficient GPU processing.\n",
    "- `prefetch()` overlaps data preprocessing and model execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Model Architecture\n",
    "Our model will be a simple Sequential model with:\n",
    "- An input layer matching our number of features.\n",
    "- Two hidden `Dense` layers with `ReLU` activation. We use `kernel_regularizer` to prevent overfitting.\n",
    "- `Dropout` layers to randomly set a fraction of input units to 0 at each update during training, which also helps prevent overfitting.\n",
    "- A final `Dense` output layer with a `sigmoid` activation function, which outputs a probability between 0 and 1, perfect for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the Model\n",
    "Before training, we must compile the model, specifying:\n",
    "- **`loss`**: The function to measure the model's error. `BinaryCrossentropy` is ideal for two-class problems.\n",
    "- **`optimizer`**: The algorithm to update the model's weights. `Adam` is a popular and effective choice.\n",
    "- **`metrics`**: The metric(s) to monitor during training. We'll use `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (train_index, val_index) in enumerate(kf.split(X_train, y)):\n",
    "    print(f'\\n----- Fold {fold + 1}/{NUM_FOLD} -----')\n",
    "\n",
    "    # Split data for current fold\n",
    "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "    # Create TensorFlow Datasets\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train_fold, y_train_fold))\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val_fold, y_val_fold))\n",
    "\n",
    "    # Optimize input pipelines\n",
    "    final_train_ds = (train_ds\n",
    "                      .cache()\n",
    "                      .shuffle(buffer_size=SHUFFLE_SIZE)\n",
    "                      .batch(batch_size=BATCH_SIZE)\n",
    "                      .prefetch(buffer_size=PREFETCH_SIZE))\n",
    "\n",
    "    final_val_ds = (val_ds\n",
    "                    .cache()\n",
    "                    .batch(batch_size=BATCH_SIZE)\n",
    "                    .prefetch(buffer_size=PREFETCH_SIZE))\n",
    "\n",
    "    # Define model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-5)\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        final_train_ds,\n",
    "        epochs=200,\n",
    "        validation_data=final_val_ds,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # Save history for visualization\n",
    "    histories.append(history.history)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_loss, val_accuracy = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "    print(f'Fold {fold + 1} Accuracy: {val_accuracy:.4f}')\n",
    "    print(f'Fold {fold + 1} Loss: {val_loss:.4f}')\n",
    "    fold_accuracies.append(val_accuracy)\n",
    "    fold_losses.append(val_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.3. Cross-Validation Summary**\n",
    "After training across all folds, we summarize the average performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nAverage Accuracy across {NUM_FOLD} Folds: {np.mean(fold_accuracies):.4f}')\n",
    "print(f'Average Loss across {NUM_FOLD} Folds: {np.mean(fold_losses):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.4. Visualizing Training History**\n",
    "Plotting the training and validation accuracy and loss helps us understand how the model learned and whether it overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot last fold's training history\n",
    "last_hist = histories[-1]\n",
    "hist_df = pd.DataFrame(last_hist)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(hist_df['accuracy'], label='Train Accuracy')\n",
    "plt.plot(hist_df['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy (Last Fold)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(hist_df['loss'], label='Train Loss')\n",
    "plt.plot(hist_df['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss (Last Fold)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Prediction & Submission**\n",
    "With our model trained, it's time to predict survival on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to predict probabilities on the scaled test data\n",
    "y_pred_probs = model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to binary predictions (0 or 1)\n",
    "# If the probability is 0.5 or greater, predict 1 (Survived), otherwise predict 0\n",
    "y_pred = (y_pred_probs >= 0.5).astype(int).flatten() # Flatten to a 1D array\n",
    "\n",
    "print(\"Sample predictions:\", y_pred[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.1. Creating the Submission File**\n",
    "Finally, we'll format our predictions into a CSV file as required by the Kaggle competition rules. The file needs two columns: `PassengerId` and `Survived`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame for the submission\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': df_test_processed['PassengerId'], # Get PassengerId from the processed test set\n",
    "    'Survived': y_pred\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file, without the pandas index\n",
    "submission.to_csv('/kaggle/working/gender_submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created successfully!\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 26502,
     "sourceId": 3136,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
