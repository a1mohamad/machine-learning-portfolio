# Titanic - Machine Learning from Disaster
### üèóÔ∏è Predicting Survival with Advanced Feature Engineering & Neural Networks

This repository contains a high-performance solution for the [Kaggle Titanic competition](https://www.kaggle.com/competitions/titanic). The project moves beyond basic survival models by utilizing an ensemble-based imputation strategy and a deep neural network (DNN) architecture.

## üöÄ Project Overview
The goal of this project is to build a predictive model that accurately determines whether a passenger survived the shipwreck based on data such as age, sex, socio-economic class, and family size.

### üìù Workflow
The analysis follows a structured machine learning pipeline:
1.  **Data Loading & Inspection**: Analyzing training and test sets to identify missing values.
2.  **Exploratory Data Analysis (EDA)**: Visualizing correlations between features and survival outcomes.
3.  **Feature Engineering**: The core of the approach, involving intelligent imputation and feature creation.
4.  **Deep Learning Modeling**: Training a Neural Network using TensorFlow/Keras.
5.  **Submission**: Generating final predictions for the Kaggle test set.

## üî¨ Technical Implementation Details

### 1. Data Processing & Missing Value Imputation
Unlike traditional methods that use mean or median filling, this project treats missing values as a sub-problem to maintain data distribution integrity:
* **Intelligent Age Imputation**: Missing `Age` values are predicted using a regression ensemble consisting of `RandomForestRegressor` and `LGBMRegressor`. This ensures age estimates are contextually accurate based on passenger class, title, and fare.
* **Categorical Handling**: Features like `Embarked` and `Sex` are processed using `OneHotEncoder` and `LabelEncoder` for neural network compatibility.
* **Feature Scaling**: All numerical inputs are normalized using `StandardScaler` to ensure efficient gradient descent.

### 2. Feature Engineering Logic
The core "intelligence" of the model comes from several engineered features:
* **Title Extraction**: Extracted from names to identify social status (e.g., Mr, Mrs, Master, Royalty), a high-signal indicator for survival.
* **Family Dynamics**: Combined `SibSp` and `Parch` to calculate total family size, identifying those traveling alone vs. in groups.
* **Ticket & Cabin Analysis**: Extracted characteristics from ticket numbers and mapped deck levels from the `Cabin` feature to capture physical location data.

### 3. Model Architecture (Neural Network)
The final predictions are generated by a Deep Neural Network built with **TensorFlow/Keras**:
* **Input Layer**: Configured to match the dimensionality of the engineered feature set.
* **Hidden Layers**: Multiple dense layers with `ReLU` activation functions to capture non-linear relationships.
* **Regularization**: Implementation of `Dropout` layers to prevent overfitting on the training data.
* **Output Layer**: A single neuron with a `Sigmoid` activation function to output survival probability.

### 4. Training & Validation Strategy
* **Optimizer**: Adam optimizer for adaptive learning rate management.
* **Loss Function**: Binary Crossentropy.
* **Validation**: Used **Stratified K-Fold Cross-Validation** to ensure the model generalizes well and maintains a consistent survival ratio across all folds.

---
**Keywords**: TensorFlow, Keras, LightGBM, Random Forest, Feature Engineering, Kaggle Titanic.